{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Digit Alignment Using Template Matching\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from PIL import Image\n",
    "\n",
    "# Target alpha and beta values\n",
    "target_alpha = 88\n",
    "target_beta = 2\n",
    "\n",
    "# Directory containing metadata files and images\n",
    "data_dir = \"data_test\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set the MLflow experiment and load the model\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "# Define PSNR calculation\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "# Transform to convert PIL image to tensor in [0,1]\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Scan metadata files\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "found_file = None\n",
    "\n",
    "# Find matching alpha and beta\n",
    "for meta_file in metadata_files:\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "        found_file = {\n",
    "            \"metadata_file\": meta_file,\n",
    "            \"index\": metadata.get('idx'),\n",
    "            \"noise_level\": metadata.get('noise_level'),\n",
    "            \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "            \"plate_number\": metadata.get('plate_number')\n",
    "        }\n",
    "        break\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "print(f\"Noise Level: {found_file['noise_level']:.2f}\")\n",
    "print(f\"Plate Number: {found_file['plate_number']}\")\n",
    "\n",
    "# Sort bounding boxes\n",
    "original_bboxes = sorted(found_file['digit_bboxes'], key=lambda bbox: bbox[0])\n",
    "\n",
    "# Load images\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "# Convert to NumPy\n",
    "original_np = original_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Align and update bounding boxes for the reconstructed image\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Extract original digit\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        # Extract aligned digit\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Compute metrics\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "# Perform alignment\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "# Visualization\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "# Draw original bounding boxes\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Prepare table\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title('Reconstructed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best PSNR Selection with Template Matching\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import sys\n",
    "import cv2\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# -----------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------\n",
    "data_dir = \"data_test\"\n",
    "moderate_noise_threshold = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -----------------------------------\n",
    "# Load Model\n",
    "# -----------------------------------\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "runs = client.search_runs(experiment_ids=experiment.experiment_id, order_by=[\"attributes.start_time DESC\"])\n",
    "run_id = runs[0].info.run_id  # Get the last run\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment {experiment.name}\")\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    \"\"\"Calculate PSNR between two [0,1] tensor images using PyTorch functions.\"\"\"\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Function to align and update bboxes using template matching per digit\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Extract original digit\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window in reconstructed image\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        # Extract aligned digit\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Compute metrics\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "# -----------------------------------\n",
    "# Compute PSNR Heatmap\n",
    "# -----------------------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "\n",
    "psnr_dict_avg = {}\n",
    "psnr_dict_worst = {}\n",
    "\n",
    "for meta_file in tqdm(metadata_files, desc=\"Processing images\", unit=\"image\"):\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    alpha, beta, noise_level = metadata['alpha'], metadata['beta'], metadata['noise_level']\n",
    "    digit_bboxes = metadata['digit_bboxes']\n",
    "\n",
    "    idx = metadata['idx']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    # Load images as tensors\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_img = model(distorted_img)\n",
    "        reconstructed_img = torch.clamp(reconstructed_img, 0, 1)\n",
    "\n",
    "    # Convert images to NumPy\n",
    "    original_np = original_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    reconstructed_np = reconstructed_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Apply template matching alignment per digit\n",
    "    psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes)\n",
    "    avg_psnr = np.mean(psnr_per_number) if psnr_per_number else 0\n",
    "    worst_psnr = min(psnr_per_number, default=0)\n",
    "\n",
    "    if (alpha, beta) not in psnr_dict_avg:\n",
    "        psnr_dict_avg[(alpha, beta)] = []\n",
    "        psnr_dict_worst[(alpha, beta)] = []\n",
    "    psnr_dict_avg[(alpha, beta)].append(avg_psnr)\n",
    "    psnr_dict_worst[(alpha, beta)].append(worst_psnr)\n",
    "\n",
    "# Average over multiple images if any\n",
    "for key in psnr_dict_avg:\n",
    "    psnr_dict_avg[key] = np.mean(psnr_dict_avg[key])\n",
    "    psnr_dict_worst[key] = np.mean(psnr_dict_worst[key])\n",
    "\n",
    "alpha_values = sorted(set(a for (a, b) in psnr_dict_avg.keys()))\n",
    "beta_values = sorted(set(b for (a, b) in psnr_dict_avg.keys()))\n",
    "num_alphas, num_betas = len(alpha_values), len(beta_values)\n",
    "\n",
    "psnr_matrix_avg = np.full((num_betas, num_alphas), np.nan)\n",
    "alpha_to_idx = {val: i for i, val in enumerate(alpha_values)}\n",
    "beta_to_idx = {val: i for i, val in enumerate(beta_values)}\n",
    "\n",
    "# Populate the PSNR matrix\n",
    "for (a, b), val in psnr_dict_avg.items():\n",
    "    psnr_matrix_avg[beta_to_idx[b], alpha_to_idx[a]] = val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image details for a given alpha,beta function\n",
    "\n",
    "# -----------------------------------\n",
    "# Function to show image details for a given alpha,beta\n",
    "# Using the same template matching approach inside show_image_details_for\n",
    "# -----------------------------------\n",
    "def show_image_details_for(alpha, beta, data_dir, model, device):\n",
    "    metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "    found_file = None\n",
    "    for meta_file in metadata_files:\n",
    "        meta_path = os.path.join(data_dir, meta_file)\n",
    "        with open(meta_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        if metadata.get('alpha') == alpha and metadata.get('beta') == beta:\n",
    "            found_file = {\n",
    "                \"metadata_file\": meta_file,\n",
    "                \"index\": metadata.get('idx'),\n",
    "                \"noise_level\": metadata.get('noise_level'),\n",
    "                \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "                \"plate_number\": metadata.get('plate_number')\n",
    "            }\n",
    "            break\n",
    "\n",
    "    if found_file is None:\n",
    "        print(f\"No images found for alpha={alpha}, beta={beta}.\")\n",
    "        return\n",
    "\n",
    "    found_file['digit_bboxes'].sort(key=lambda bbox: bbox[0])\n",
    "\n",
    "    idx = found_file['index']\n",
    "    original_image_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_image_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_tensor = model(distorted_img)\n",
    "        reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    # Align and update bboxes with template matching\n",
    "    def calc_psnr_ssim(original_image, reconstructed_image, digit_bboxes):\n",
    "        # We'll reuse align_and_update_bboxes here to get aligned results\n",
    "        psnr_vals, ssim_vals, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes)\n",
    "        return psnr_vals, ssim_vals, updated_bboxes\n",
    "\n",
    "    psnr_per_number, ssim_per_number, updated_bboxes = calc_psnr_ssim(original_img, reconstructed_tensor, found_file['digit_bboxes'])\n",
    "\n",
    "    reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "    reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "    original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "    for i, bbox in enumerate(found_file['digit_bboxes'], start=1):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "        cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "    for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "        cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "    original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "    reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "    for i, (p, s) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "        table_data.append([str(i), f\"{p:.2f}\", f\"{s:.3f}\"])\n",
    "    transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "    fig2 = plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(original_image_rgb)\n",
    "    plt.title(f'Original Image (Alpha={alpha}, Beta={beta})')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.imshow(reconstructed_image_rgb)\n",
    "    plt.title('Reconstructed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    table = plt.table(cellText=transposed_table_data,\n",
    "                      cellLoc='center',\n",
    "                      loc='center',\n",
    "                      bbox=[0, -0.55, 1, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# Display Heatmap and Add Click Event\n",
    "# -----------------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(psnr_matrix_avg, origin='lower', aspect='auto', cmap=\"viridis\")\n",
    "plt.title(\"Average PSNR per Digit\")\n",
    "plt.colorbar(label='PSNR (dB)')\n",
    "plt.xticks(range(0, num_alphas, 5), alpha_values[::5])\n",
    "plt.yticks(range(0, num_betas, 5), beta_values[::5])\n",
    "plt.xlabel(\"Alpha (degrees)\")\n",
    "plt.ylabel(\"Beta (degrees)\")\n",
    "\n",
    "def format_coord(x, y):\n",
    "    col = int(round(x))\n",
    "    row = int(round(y))\n",
    "    if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "        alpha = alpha_values[col]\n",
    "        beta = beta_values[row]\n",
    "        psnr_value = psnr_matrix_avg[row, col]\n",
    "        return f\"Alpha: {alpha:.0f}, Beta: {beta:.0f}, PSNR: {psnr_value:.2f} dB\" if not np.isnan(psnr_value) else f\"Alpha: {alpha:.0f}, Beta: {beta:.0f}, PSNR: N/A\"\n",
    "    return \"Alpha: N/A, Beta: N/A\"\n",
    "\n",
    "plt.gca().format_coord = format_coord\n",
    "\n",
    "def on_click(event):\n",
    "    if event.inaxes == plt.gca():\n",
    "        x, y = event.xdata, event.ydata\n",
    "        if x is None or y is None:\n",
    "            return\n",
    "        col = int(round(x))\n",
    "        row = int(round(y))\n",
    "        if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "            alpha = alpha_values[col]\n",
    "            beta = beta_values[row]\n",
    "            show_image_details_for(alpha, beta, data_dir, model, device)\n",
    "\n",
    "plt.gcf().canvas.mpl_connect('button_press_event', on_click)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paddle OCR with Global Template Matching and Bounding Box Alignment\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------\n",
    "\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize PaddleOCR (English, digits only, low confidence allowed)\n",
    "ocr = PaddleOCR(\n",
    "    use_angle_cls=False,  \n",
    "    lang='en',\n",
    "    use_space_char=False,\n",
    "    drop_score=0.1,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Load Model via MLflow\n",
    "# -----------------------------------------------------\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Locate Metadata for Given Alpha, Beta\n",
    "# -----------------------------------------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "found_file = None\n",
    "for meta_file in metadata_files:\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "        found_file = {\n",
    "            \"metadata_file\": meta_file,\n",
    "            \"index\": metadata.get('idx'),\n",
    "            \"noise_level\": metadata.get('noise_level'),\n",
    "            \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "            \"plate_number\": metadata.get('plate_number')\n",
    "        }\n",
    "        break\n",
    "\n",
    "if not found_file:\n",
    "    print(\"No image found for given alpha, beta.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "print(f\"Noise Level: {found_file['noise_level']:.2f}\")\n",
    "print(f\"Plate Number: {found_file['plate_number']}\")\n",
    "\n",
    "# Sort original bboxes by x to ensure left-to-right order\n",
    "original_bboxes = sorted(found_file['digit_bboxes'], key=lambda bbox: bbox[0])\n",
    "\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    \"\"\"\n",
    "    Template matching to find updated bounding boxes for each digit.\n",
    "    \"\"\"\n",
    "    from pytorch_msssim import ssim\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x = max_loc[0] + search_x1\n",
    "        best_y = max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Run PaddleOCR on the reconstructed image\n",
    "# -----------------------------------------------------\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show_bgr = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "ocr_results = ocr.ocr(reconstructed_show_bgr, cls=False)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Process OCR Results:\n",
    "# Filter digits only, and align them with updated_bboxes\n",
    "# -----------------------------------------------------\n",
    "def extract_digits_from_ocr(ocr_results):\n",
    "    \"\"\"\n",
    "    Extracts per-character bounding boxes for digits from PaddleOCR line results.\n",
    "    If a line says \"3163\" with one bounding box, we split horizontally.\n",
    "    Returns a list of dicts: {\"char\": c, \"bbox\": (x1,y1,x2,y2)} for each digit found.\n",
    "    \"\"\"\n",
    "    per_char_results = []\n",
    "\n",
    "    for line in ocr_results[0]:\n",
    "        box, (text, conf) = line\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            continue\n",
    "        text = text.strip()\n",
    "\n",
    "        # Keep only digits from this text\n",
    "        filtered_text = \"\".join([ch for ch in text if ch.isdigit()])\n",
    "        if len(filtered_text) == 0:\n",
    "            continue\n",
    "\n",
    "        xs = [p[0] for p in box]\n",
    "        ys = [p[1] for p in box]\n",
    "        min_x, max_x = min(xs), max(xs)\n",
    "        min_y, max_y = min(ys), max(ys)\n",
    "\n",
    "        num_chars = len(text)\n",
    "        # We'll subdivide bounding box by number of characters to approximate char positions\n",
    "        # But we only consider digits. This means we must map digits back to their positions\n",
    "        # in the original text.\n",
    "\n",
    "        # Approach: for each character in text, assign a sub-box. Then only keep if it's a digit.\n",
    "        width = max_x - min_x\n",
    "        char_width = width / num_chars if num_chars > 0 else width\n",
    "\n",
    "        current_x = min_x\n",
    "        for i, ch in enumerate(text):\n",
    "            c_x1 = int(min_x + i * char_width)\n",
    "            c_x2 = int(min_x + (i+1)* char_width)\n",
    "            c_y1 = int(min_y)\n",
    "            c_y2 = int(max_y)\n",
    "\n",
    "            if ch.isdigit():\n",
    "                per_char_results.append({\n",
    "                    \"char\": ch,\n",
    "                    \"bbox\": (c_x1, c_y1, c_x2, c_y2)\n",
    "                })\n",
    "\n",
    "    return per_char_results\n",
    "\n",
    "char_results = []\n",
    "if ocr_results and len(ocr_results) > 0 and ocr_results[0] is not None:\n",
    "    char_results = extract_digits_from_ocr(ocr_results)\n",
    "\n",
    "# Sort recognized digits by x-coordinate\n",
    "char_results.sort(key=lambda r: r[\"bbox\"][0])  # sort by left x\n",
    "\n",
    "recognized_chars = [r[\"char\"] for r in char_results]\n",
    "recognized_text = \"\".join(recognized_chars)\n",
    "\n",
    "# We know we need exactly 6 digits:\n",
    "plate_number = found_file['plate_number']\n",
    "assert len(plate_number) == 6, \"Plate number must have exactly 6 digits\"\n",
    "\n",
    "# If we have more than 6 recognized digits, take only first 6 (leftmost)\n",
    "if len(recognized_chars) > 6:\n",
    "    recognized_chars = recognized_chars[:6]\n",
    "    char_results = char_results[:6]\n",
    "\n",
    "# If fewer than 6 digits recognized, fill missing with '?'\n",
    "if len(recognized_chars) < 6:\n",
    "    missing = 6 - len(recognized_chars)\n",
    "    recognized_chars.extend(['?'] * missing)\n",
    "\n",
    "final_recognized_text = \"\".join(recognized_chars[:6])\n",
    "\n",
    "# Compute accuracy:\n",
    "correct_digits = sum(1 for a, b in zip(plate_number, final_recognized_text) if a == b)\n",
    "accuracy = correct_digits / 6.0\n",
    "\n",
    "print(f\"\\nPlate Number (GT): {plate_number}\")\n",
    "print(f\"Recognized (final): {final_recognized_text}\")\n",
    "print(f\"OCR Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Visualization\n",
    "# -----------------------------------------------------\n",
    "# Draw original bounding boxes on original image\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "reconstructed_show_bgr = reconstructed_show_bgr.copy()\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show_bgr, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show_bgr, str(i), (x, y-5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw recognized characters on reconstructed image\n",
    "for r in char_results:\n",
    "    c = r[\"char\"]\n",
    "    x1, y1, x2, y2 = r[\"bbox\"]\n",
    "    cv2.rectangle(reconstructed_show_bgr, (x1,y1), (x2,y2), (0,255,0), 1)\n",
    "    cv2.putText(reconstructed_show_bgr, c, (x1,y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0),1)\n",
    "\n",
    "# Prepare table for PSNR, SSIM\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image with Original BBoxes')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title(f\"Reconstructed Image with Updated BBoxes and OCR Results\\nRecognized: {final_recognized_text}, Accuracy: {accuracy*100:.2f}%\")\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "\n",
    "plt.subplots_adjust(left=0.2, bottom=0.3, top=0.9)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Digit Paddle OCR with Template Matching and Margin Adjustment\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from paddleocr import PaddleOCR\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# --------------------\n",
    "# Configuration\n",
    "# --------------------\n",
    "\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "search_margin = 10  # Margin for template matching search\n",
    "ocr_margin = 17      # Margin around TMBB for OCR\n",
    "\n",
    "digit_dict = \"digit_dict.txt\"\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(\n",
    "    use_angle_cls=True,\n",
    "    lang='en',\n",
    "    rec_char_dict_path=digit_dict,\n",
    "    use_space_char=False,\n",
    "    drop_score=0\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Load the Model\n",
    "# --------------------\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "runs = client.search_runs(experiment_ids=experiment.experiment_id, order_by=[\"attributes.start_time DESC\"])\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval().to(device)\n",
    "\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Find Matching alpha, beta\n",
    "# --------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "found_file = None\n",
    "for meta_file in metadata_files:\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "        found_file = {\n",
    "            \"metadata_file\": meta_file,\n",
    "            \"index\": metadata.get('idx'),\n",
    "            \"noise_level\": metadata.get('noise_level'),\n",
    "            \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "            \"plate_number\": metadata.get('plate_number')\n",
    "        }\n",
    "        break\n",
    "\n",
    "if not found_file:\n",
    "    print(\"Image with the specified alpha and beta not found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "print(f\"Noise Level: {found_file['noise_level']:.2f}\")\n",
    "print(f\"Plate Number: {found_file['plate_number']}\")\n",
    "\n",
    "original_bboxes = sorted(found_file['digit_bboxes'], key=lambda bbox: bbox[0])\n",
    "\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    from pytorch_msssim import ssim\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Search region\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "# Draw original bounding boxes\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# --------------------\n",
    "# Per-Digit OCR using TMBB and PaddleOCR\n",
    "# --------------------\n",
    "def run_ocr_on_digit(reconstructed_bgr, bbox, margin=5):\n",
    "    x, y, w, h = bbox\n",
    "    H, W, _ = reconstructed_bgr.shape\n",
    "    x1 = max(0, x - margin)\n",
    "    y1 = max(0, y - margin)\n",
    "    x2 = min(W, x + w + margin)\n",
    "    y2 = min(H, y + h + margin)\n",
    "\n",
    "    digit_roi = reconstructed_bgr[y1:y2, x1:x2]\n",
    "    if digit_roi.size == 0:\n",
    "        print(f\"Warning: Empty ROI for bbox {bbox}\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"\\nRunning OCR on region: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "\n",
    "    try:\n",
    "        ocr_results = ocr.ocr(digit_roi, cls=False)\n",
    "        print(f\"OCR Raw Results: {ocr_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OCR: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    recognized_digit = \"\"\n",
    "    if ocr_results and ocr_results[0]:\n",
    "        for line in ocr_results[0]:\n",
    "            box, text_data = line[0], line[1]\n",
    "            recognized_text, confidence = text_data\n",
    "            filtered = \"\".join(ch for ch in recognized_text if ch.isdigit())\n",
    "            if filtered:\n",
    "                recognized_digit = filtered[0]\n",
    "                print(f\"  Detected: '{recognized_digit}' (Confidence: {confidence:.2f})\")\n",
    "                break\n",
    "\n",
    "    if recognized_digit:\n",
    "        print(f\"Final Recognized Digit: {recognized_digit}\")\n",
    "    else:\n",
    "        print(\"No valid digits recognized.\")\n",
    "    return recognized_digit\n",
    "\n",
    "\n",
    "\n",
    "recognized_digits = []\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    digit = run_ocr_on_digit(reconstructed_show, bbox, margin=ocr_margin)\n",
    "    if not digit:\n",
    "        digit = \"?\"  # If no digit recognized, use placeholder\n",
    "    recognized_digits.append(digit)\n",
    "\n",
    "recognized_text = \"\".join(recognized_digits)\n",
    "ground_truth = found_file['plate_number']\n",
    "correct_digits = sum(1 for a, b in zip(ground_truth, recognized_text) if a == b)\n",
    "ocr_accuracy = correct_digits / len(ground_truth) if ground_truth else 0.0\n",
    "\n",
    "print(f\"Recognized Digits (Left to Right): {recognized_text}\")\n",
    "print(f\"OCR Accuracy: {ocr_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Draw per-digit OCR results\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    digit = recognized_digits[i-1]\n",
    "    cv2.putText(reconstructed_image_rgb, digit, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "# --------------------\n",
    "# Visualization\n",
    "# --------------------\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title(f\"Reconstructed Image - OCR: {recognized_text}, Accuracy: {ocr_accuracy*100:.2f}%\")\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_alpha = 88  # Target alpha\n",
    "target_beta = 3  # Target beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit-Level Tesseract OCR with Template Matching and Tesseract Integration\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# --------------------\n",
    "# Configuration\n",
    "# --------------------\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "target_alpha = 88  # Target alpha\n",
    "target_beta = 4  # Target beta\n",
    "\n",
    "# Set the MLflow experiment and load the model\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Find metadata for the target alpha, beta\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "found_file = None\n",
    "for meta_file in metadata_files:\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "        found_file = {\n",
    "            \"metadata_file\": meta_file,\n",
    "            \"index\": metadata.get('idx'),\n",
    "            \"noise_level\": metadata.get('noise_level'),\n",
    "            \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "            \"plate_number\": metadata.get('plate_number')\n",
    "        }\n",
    "        break\n",
    "\n",
    "if not found_file:\n",
    "    print(\"Image with the specified alpha and beta not found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "print(f\"Noise Level: {found_file['noise_level']:.2f}\")\n",
    "print(f\"Plate Number: {found_file['plate_number']}\")\n",
    "\n",
    "original_bboxes = sorted(found_file['digit_bboxes'], key=lambda bbox: bbox[0])\n",
    "\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "# Align and update bboxes using template matching\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Extract original digit\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window in reconstructed\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        # Compute PSNR and SSIM\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "# Tesseract single-digit OCR function\n",
    "def ocr_single_digit(image_bgr):\n",
    "    \"\"\"\n",
    "    Recognize a single digit from a small cropped patch using Tesseract.\n",
    "    Uses --psm 10 for single char and a whitelist of digits.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Tesseract config for single character digit recognition\n",
    "    config = r'--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789'\n",
    "\n",
    "    text = pytesseract.image_to_string(thresh, config=config).strip()\n",
    "    if len(text) == 1 and text.isdigit():\n",
    "        return text\n",
    "    # If Tesseract fails or returns something unexpected, return '?'\n",
    "    return '?'\n",
    "\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "# Convert reconstructed to BGR for visualization\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "# Draw original bounding boxes\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Perform per-digit OCR using TMBB\n",
    "plate_number_gt = found_file['plate_number']\n",
    "recognized_digits = []\n",
    "\n",
    "for i, bbox in enumerate(updated_bboxes):\n",
    "    x, y, w, h = bbox\n",
    "    # Add margin\n",
    "    margin = 17\n",
    "    x1 = max(0, x - margin)\n",
    "    y1 = max(0, y - margin)\n",
    "    x2 = min(reconstructed_show.shape[1], x + w + margin)\n",
    "    y2 = min(reconstructed_show.shape[0], y + h + margin)\n",
    "    digit_patch = reconstructed_show[y1:y2, x1:x2]\n",
    "    \n",
    "    reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "    cv2.rectangle(reconstructed_show, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    recognized_digit = ocr_single_digit(digit_patch)\n",
    "    recognized_digits.append(recognized_digit)\n",
    "\n",
    "recognized_text = \"\".join(recognized_digits)\n",
    "\n",
    "# Compute accuracy\n",
    "gt = plate_number_gt\n",
    "correct_digits = sum(1 for a, b in zip(gt, recognized_text) if a == b)\n",
    "accuracy = correct_digits / len(gt) if gt else 0.0\n",
    "\n",
    "# Prepare table\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title(f'Reconstructed Image\\nGT: {gt}, Recognized: {recognized_text}, Acc: {accuracy*100:.2f}%')\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Image Analysis with Metadata Retrieval and Visualization\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import cv2\n",
    "from pytorch_msssim import ssim\n",
    "import pytesseract\n",
    "\n",
    "# --------------------\n",
    "# Configuration\n",
    "# --------------------\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# MLflow model load (if needed; assuming similar to Code 1)\n",
    "mlflow.set_experiment('Unet')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet')\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval().to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_alpha = 78  # Target alpha\n",
    "target_beta = 69  # Target beta\n",
    "# --------------------\n",
    "# Functions \n",
    "# --------------------\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "def find_metadata(data_dir, target_alpha, target_beta):\n",
    "    \"\"\"\n",
    "    Find and return metadata for the specified alpha and beta.\n",
    "    \"\"\"\n",
    "    metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "    for meta_file in metadata_files:\n",
    "        meta_path = os.path.join(data_dir, meta_file)\n",
    "        with open(meta_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "            return {\n",
    "                \"metadata_file\": meta_file,\n",
    "                \"index\": metadata.get('idx'),\n",
    "                \"noise_level\": metadata.get('noise_level'),\n",
    "                \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "                \"plate_number\": metadata.get('plate_number')\n",
    "            }\n",
    "    return None\n",
    "\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        # Compute PSNR and SSIM\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "def ocr_single_digit(image_bgr):\n",
    "    \"\"\"\n",
    "    Recognize a single digit using Tesseract with single char mode and digit whitelist.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "    config = r'--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789'\n",
    "    text = pytesseract.image_to_string(thresh, config=config).strip()\n",
    "    if len(text) == 1 and text.isdigit():\n",
    "        return text\n",
    "    return '?'\n",
    "\n",
    "def compute_ocr_metrics(image_bgr, updated_bboxes, plate_number_gt, margin):\n",
    "    recognized_digits = []\n",
    "    for (x, y, w, h) in updated_bboxes:\n",
    "        x1 = max(0, x - margin)\n",
    "        y1 = max(0, y - margin)\n",
    "        x2 = min(image_bgr.shape[1], x + w + margin)\n",
    "        y2 = min(image_bgr.shape[0], y + h + margin)\n",
    "        \n",
    "        digit_patch = image_bgr[y1:y2, x1:x2]\n",
    "        recognized_digit = ocr_single_digit(digit_patch)\n",
    "        recognized_digits.append(recognized_digit)\n",
    "\n",
    "    recognized_text = \"\".join(recognized_digits)\n",
    "    gt = plate_number_gt\n",
    "    correct_digits = sum(1 for a, b in zip(gt, recognized_text) if a == b)\n",
    "    ocr_accuracy = correct_digits / len(gt) if len(gt) > 0 else 0.0\n",
    "    ocr_binary = 1.0 if recognized_text == gt else 0.0\n",
    "    return recognized_text, ocr_accuracy, ocr_binary\n",
    "\n",
    "# --------------------\n",
    "# Main Processing\n",
    "# --------------------\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Find metadata for the target alpha and beta\n",
    "found_file = find_metadata(data_dir, target_alpha, target_beta)\n",
    "\n",
    "if not found_file:\n",
    "    print(\"Image with the specified alpha and beta not found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "\n",
    "original_bboxes = found_file['digit_bboxes']\n",
    "\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "if not (os.path.exists(original_image_path) and os.path.exists(distorted_image_path)):\n",
    "    print(\"Original or distorted image not found.\")\n",
    "    exit()\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "# Convert reconstructed to BGR for visualization\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "# Draw original bounding boxes\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Perform OCR and compute metrics\n",
    "plate_number_gt = found_file['plate_number']\n",
    "recognized_text, ocr_accuracy, ocr_binary = compute_ocr_metrics(reconstructed_show, updated_bboxes, plate_number_gt, margin=2)\n",
    "\n",
    "# Prepare table data\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title(f'Reconstructed Image (alpha: {target_alpha}, beta: {target_beta})\\nGT: {plate_number_gt}, Recognized: {recognized_text}, Acc: {ocr_accuracy*100:.2f}%')\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap Visualization of average PSNR, SSIM, and OCR Metrics with Interactive Analysis\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import cv2\n",
    "from pytorch_msssim import ssim\n",
    "import pytesseract\n",
    "\n",
    "# --------------------\n",
    "# Configuration\n",
    "# --------------------\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# MLflow model load\n",
    "mlflow.set_experiment('Unet')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet')\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval().to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "def ocr_single_digit(image_bgr):\n",
    "    \"\"\"\n",
    "    Recognize a single digit using Tesseract with single char mode and digit whitelist.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "    config = r'--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789'\n",
    "    text = pytesseract.image_to_string(thresh, config=config).strip()\n",
    "    if len(text) == 1 and text.isdigit():\n",
    "        return text\n",
    "    return '?'\n",
    "\n",
    "def compute_ocr_metrics(reconstructed_bgr, updated_bboxes, plate_number_gt):\n",
    "    recognized_digits = []\n",
    "    M = 16\n",
    "    for (x, y, w, h) in updated_bboxes:\n",
    "        x1 = max(0, x - M)\n",
    "        y1 = max(0, y - M)\n",
    "        x2 = min(reconstructed_bgr.shape[1], x + w + M)\n",
    "        y2 = min(reconstructed_bgr.shape[0], y + h + M)\n",
    "        digit_patch = reconstructed_bgr[y1:y2, x1:x2]\n",
    "        recognized_digit = ocr_single_digit(digit_patch)\n",
    "        recognized_digits.append(recognized_digit)\n",
    "\n",
    "    recognized_text = \"\".join(recognized_digits)\n",
    "    gt = plate_number_gt\n",
    "    correct_digits = sum(1 for a, b in zip(gt, recognized_text) if a == b)\n",
    "    ocr_accuracy = correct_digits / len(gt) if len(gt) > 0 else 0.0\n",
    "    ocr_binary = 1.0 if recognized_text == gt else 0.0\n",
    "    return recognized_text, ocr_accuracy, ocr_binary\n",
    "\n",
    "# -----------------------------------\n",
    "# Compute metrics for each (alpha,beta)\n",
    "# -----------------------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "\n",
    "psnr_dict_avg = {}\n",
    "ssim_dict_avg = {}\n",
    "ocr_acc_dict_avg = {}\n",
    "ocr_bin_dict_avg = {}\n",
    "\n",
    "for meta_file in tqdm(metadata_files, desc=\"Processing images\", unit=\"image\"):\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    alpha, beta = metadata['alpha'], metadata['beta']\n",
    "    digit_bboxes = metadata['digit_bboxes']\n",
    "    plate_number_gt = metadata['plate_number']\n",
    "\n",
    "    idx = metadata['idx']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    if not (os.path.exists(original_path) and os.path.exists(distorted_path)):\n",
    "        continue\n",
    "\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_img = model(distorted_img)\n",
    "        reconstructed_img = torch.clamp(reconstructed_img, 0, 1)\n",
    "\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    reconstructed_np = reconstructed_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes)\n",
    "    avg_psnr = np.mean(psnr_per_number) if psnr_per_number else 0.0\n",
    "    avg_ssim = np.mean(ssim_per_number) if ssim_per_number else 0.0\n",
    "\n",
    "    image_bgr = (reconstructed_np * 255).astype(np.uint8)\n",
    "    image_bgr = cv2.cvtColor(image_bgr, cv2.COLOR_RGB2BGR)\n",
    "    recognized_text, ocr_accuracy, ocr_binary = compute_ocr_metrics(image_bgr, updated_bboxes, plate_number_gt)\n",
    "\n",
    "    if (alpha, beta) not in psnr_dict_avg:\n",
    "        psnr_dict_avg[(alpha, beta)] = []\n",
    "        ssim_dict_avg[(alpha, beta)] = []\n",
    "        ocr_acc_dict_avg[(alpha, beta)] = []\n",
    "        ocr_bin_dict_avg[(alpha, beta)] = []\n",
    "\n",
    "    psnr_dict_avg[(alpha, beta)].append(avg_psnr)\n",
    "    ssim_dict_avg[(alpha, beta)].append(avg_ssim)\n",
    "    ocr_acc_dict_avg[(alpha, beta)].append(ocr_accuracy)\n",
    "    ocr_bin_dict_avg[(alpha, beta)].append(ocr_binary)\n",
    "\n",
    "# Average if multiple images per angle (if any)\n",
    "for key in psnr_dict_avg:\n",
    "    psnr_dict_avg[key] = np.mean(psnr_dict_avg[key])\n",
    "    ssim_dict_avg[key] = np.mean(ssim_dict_avg[key])\n",
    "    ocr_acc_dict_avg[key] = np.mean(ocr_acc_dict_avg[key])\n",
    "    ocr_bin_dict_avg[key] = np.mean(ocr_bin_dict_avg[key])\n",
    "\n",
    "alpha_values = sorted(set(a for (a, b) in psnr_dict_avg.keys()))\n",
    "beta_values = sorted(set(b for (a, b) in psnr_dict_avg.keys()))\n",
    "num_alphas, num_betas = len(alpha_values), len(beta_values)\n",
    "\n",
    "def create_matrix_from_dict(data_dict):\n",
    "    mat = np.full((num_betas, num_alphas), np.nan)\n",
    "    alpha_to_idx = {val: i for i, val in enumerate(alpha_values)}\n",
    "    beta_to_idx = {val: i for i, val in enumerate(beta_values)}\n",
    "    for (a, b), val in data_dict.items():\n",
    "        mat[beta_to_idx[b], alpha_to_idx[a]] = val\n",
    "    return mat\n",
    "\n",
    "psnr_matrix_avg = create_matrix_from_dict(psnr_dict_avg)\n",
    "ssim_matrix_avg = create_matrix_from_dict(ssim_dict_avg)\n",
    "ocr_acc_matrix = create_matrix_from_dict(ocr_acc_dict_avg)\n",
    "ocr_bin_matrix = create_matrix_from_dict(ocr_bin_dict_avg)\n",
    "\n",
    "# -----------------------------------\n",
    "# Interactive Plot with Buttons\n",
    "# -----------------------------------\n",
    "current_metric = 'PSNR'\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "plt.subplots_adjust(bottom=0.2)  # space for buttons\n",
    "\n",
    "im = ax.imshow(psnr_matrix_avg, origin='lower', aspect='auto', cmap=\"viridis\")\n",
    "ax.set_title(\"Average PSNR per Digit\")\n",
    "cb = plt.colorbar(im, ax=ax, label='PSNR (dB)')\n",
    "ax.set_xticks(range(0, num_alphas, 5))\n",
    "ax.set_xticklabels(alpha_values[::5])\n",
    "ax.set_yticks(range(0, num_betas, 5))\n",
    "ax.set_yticklabels(beta_values[::5])\n",
    "ax.set_xlabel(\"Alpha (degrees)\")\n",
    "ax.set_ylabel(\"Beta (degrees)\")\n",
    "\n",
    "def format_coord(x, y):\n",
    "    col = int(round(x))\n",
    "    row = int(round(y))\n",
    "    if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "        alpha = alpha_values[col]\n",
    "        beta = beta_values[row]\n",
    "        if current_metric == 'PSNR':\n",
    "            val = psnr_matrix_avg[row, col]\n",
    "            return f\"Alpha: {alpha}, Beta: {beta}, PSNR: {val:.2f} dB\" if not np.isnan(val) else \"N/A\"\n",
    "        elif current_metric == 'SSIM':\n",
    "            val = ssim_matrix_avg[row, col]\n",
    "            return f\"Alpha: {alpha}, Beta: {beta}, SSIM: {val:.3f}\" if not np.isnan(val) else \"N/A\"\n",
    "        elif current_metric == 'OCR_Accuracy':\n",
    "            val = ocr_acc_matrix[row, col]\n",
    "            return f\"Alpha: {alpha}, Beta: {beta}, OCR Acc: {val*100:.2f}%\" if not np.isnan(val) else \"N/A\"\n",
    "        elif current_metric == 'OCR_Binary':\n",
    "            val = ocr_bin_matrix[row, col]\n",
    "            return f\"Alpha: {alpha}, Beta: {beta}, OCR Binary: {val:.0f}\" if not np.isnan(val) else \"N/A\"\n",
    "    return \"N/A\"\n",
    "\n",
    "ax.format_coord = format_coord\n",
    "\n",
    "def show_image_details_for(alpha, beta):\n",
    "    # Re-run the detailed view logic\n",
    "    # Find the file again\n",
    "    found_file = None\n",
    "    for meta_file in metadata_files:\n",
    "        meta_path = os.path.join(data_dir, meta_file)\n",
    "        with open(meta_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        if metadata['alpha'] == alpha and metadata['beta'] == beta:\n",
    "            found_file = metadata\n",
    "            break\n",
    "\n",
    "    if found_file is None:\n",
    "        print(\"No images found for that angle.\")\n",
    "        return\n",
    "\n",
    "    found_file['digit_bboxes'].sort(key=lambda bbox: bbox[0])\n",
    "    idx = found_file['idx']\n",
    "    plate_number_gt = found_file['plate_number']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        reconstructed_tensor = model(distorted_img)\n",
    "        reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    psnr_vals, ssim_vals, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, found_file['digit_bboxes'])\n",
    "\n",
    "    reconstructed_bgr = (reconstructed_np*255).astype(np.uint8)\n",
    "    reconstructed_bgr = cv2.cvtColor(reconstructed_bgr, cv2.COLOR_RGB2BGR)\n",
    "    recognized_text, ocr_accuracy, ocr_binary = compute_ocr_metrics(reconstructed_bgr, updated_bboxes, plate_number_gt)\n",
    "\n",
    "    original_image_cv = cv2.imread(original_path)\n",
    "    for i, bbox in enumerate(found_file['digit_bboxes'], start=1):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0,0,255),1)\n",
    "        cv2.putText(original_image_cv, str(i), (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,150,0),1)\n",
    "    original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    reconstructed_show = reconstructed_bgr.copy()\n",
    "    for i,bbox in enumerate(updated_bboxes, start=1):\n",
    "        x,y,w,h = bbox\n",
    "        cv2.rectangle(reconstructed_show, (x,y),(x+w,y+h),(0,0,255),1)\n",
    "        cv2.putText(reconstructed_show,str(i),(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,150,0),1)\n",
    "    reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    table_data = [[\"Digit\",\"PSNR(dB)\",\"SSIM\"]]\n",
    "    for i,(p,s) in enumerate(zip(psnr_vals, ssim_vals), start=1):\n",
    "        table_data.append([str(i), f\"{p:.2f}\", f\"{s:.3f}\"])\n",
    "    transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "    fig2 = plt.figure(figsize=(14,7))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(original_image_rgb)\n",
    "    plt.title(f'Original Image (Alpha={alpha}, Beta={beta})')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(reconstructed_image_rgb)\n",
    "    plt.title(f'Reconstructed Image\\nGT: {plate_number_gt}, Rec: {recognized_text}, OCR Acc: {ocr_accuracy*100:.2f}%, Binary: {ocr_binary}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    table = plt.table(cellText=transposed_table_data,\n",
    "                      cellLoc='center',\n",
    "                      loc='center',\n",
    "                      bbox=[0,-0.55,1,0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def on_click(event):\n",
    "    if event.inaxes == ax:\n",
    "        x, y = event.xdata, event.ydata\n",
    "        if x is None or y is None:\n",
    "            return\n",
    "        col = int(round(x))\n",
    "        row = int(round(y))\n",
    "        if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "            alpha = alpha_values[col]\n",
    "            beta = beta_values[row]\n",
    "            show_image_details_for(alpha, beta)\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "\n",
    "# Add buttons to switch between metrics\n",
    "ax_psnr = plt.axes([0.1, 0.05, 0.1, 0.05])\n",
    "ax_ssim = plt.axes([0.22, 0.05, 0.1, 0.05])\n",
    "ax_ocr_acc = plt.axes([0.34, 0.05, 0.12, 0.05])\n",
    "ax_ocr_bin = plt.axes([0.48, 0.05, 0.1, 0.05])\n",
    "\n",
    "btn_psnr = Button(ax_psnr, 'PSNR')\n",
    "btn_ssim = Button(ax_ssim, 'SSIM')\n",
    "btn_ocr_acc = Button(ax_ocr_acc, 'OCR Acc')\n",
    "btn_ocr_bin = Button(ax_ocr_bin, 'OCR Bin')\n",
    "\n",
    "def update_heatmap(metric):\n",
    "    global current_metric\n",
    "    current_metric = metric\n",
    "    ax.clear()\n",
    "    if metric == 'PSNR':\n",
    "        data = psnr_matrix_avg\n",
    "        title = \"Average PSNR per Digit\"\n",
    "        cbar_label = \"PSNR (dB)\"\n",
    "    elif metric == 'SSIM':\n",
    "        data = ssim_matrix_avg\n",
    "        title = \"Average SSIM per Digit\"\n",
    "        cbar_label = \"SSIM\"\n",
    "    elif metric == 'OCR_Accuracy':\n",
    "        data = ocr_acc_matrix\n",
    "        title = \"Average OCR Accuracy\"\n",
    "        cbar_label = \"OCR Acc\"\n",
    "    else:\n",
    "        data = ocr_bin_matrix\n",
    "        title = \"OCR Binary (1=All Correct)\"\n",
    "        cbar_label = \"OCR Binary\"\n",
    "\n",
    "    im = ax.imshow(data, origin='lower', aspect='auto', cmap='viridis')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(0,num_alphas,5))\n",
    "    ax.set_xticklabels(alpha_values[::5])\n",
    "    ax.set_yticks(range(0,num_betas,5))\n",
    "    ax.set_yticklabels(beta_values[::5])\n",
    "    ax.set_xlabel(\"Alpha (degrees)\")\n",
    "    ax.set_ylabel(\"Beta (degrees)\")\n",
    "    ax.format_coord = format_coord\n",
    "    fig.colorbar(im, ax=ax, label=cbar_label)\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "def on_psnr_clicked(event):\n",
    "    update_heatmap('PSNR')\n",
    "\n",
    "def on_ssim_clicked(event):\n",
    "    update_heatmap('SSIM')\n",
    "\n",
    "def on_ocr_acc_clicked(event):\n",
    "    update_heatmap('OCR_Accuracy')\n",
    "\n",
    "def on_ocr_bin_clicked(event):\n",
    "    update_heatmap('OCR_Binary')\n",
    "\n",
    "btn_psnr.on_clicked(on_psnr_clicked)\n",
    "btn_ssim.on_clicked(on_ssim_clicked)\n",
    "btn_ocr_acc.on_clicked(on_ocr_acc_clicked)\n",
    "btn_ocr_bin.on_clicked(on_ocr_bin_clicked)\n",
    "\n",
    "current_metric = 'PSNR'  # default\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap Visualization of worst PSNR, worst SSIM, and OCR Metrics with Interactive Analysis\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import cv2\n",
    "from pytorch_msssim import ssim\n",
    "import pytesseract\n",
    "\n",
    "# --------------------\n",
    "# Configuration\n",
    "# --------------------\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# MLflow model load\n",
    "mlflow.set_experiment('Unet')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet')\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval().to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "# --------------------\n",
    "# Functions \n",
    "# --------------------\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "def align_and_update_bboxes(original_np, generated_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(generated_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(generated_np.shape[0], y + h + search_margin)\n",
    "        search_region = generated_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        # Compute PSNR and SSIM\n",
    "        aligned_digit = generated_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "def ocr_single_digit(image_bgr):\n",
    "    \"\"\"\n",
    "    Recognize a single digit using Tesseract with single char mode and digit whitelist.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "    config = r'--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789'\n",
    "    text = pytesseract.image_to_string(thresh, config=config).strip()\n",
    "    if len(text) == 1 and text.isdigit():\n",
    "        return text\n",
    "    return '?'\n",
    "\n",
    "def compute_ocr_metrics(image_bgr, updated_bboxes, plate_number_gt, margin):\n",
    "    recognized_digits = []\n",
    "    for (x, y, w, h) in updated_bboxes:\n",
    "        x1 = max(0, x - margin)\n",
    "        y1 = max(0, y - margin)\n",
    "        x2 = min(image_bgr.shape[1], x + w + margin)\n",
    "        y2 = min(image_bgr.shape[0], y + h + margin)\n",
    "        \n",
    "        digit_patch = image_bgr[y1:y2, x1:x2]\n",
    "        recognized_digit = ocr_single_digit(digit_patch)\n",
    "        recognized_digits.append(recognized_digit)\n",
    "\n",
    "    recognized_text = \"\".join(recognized_digits)\n",
    "    gt = plate_number_gt\n",
    "    correct_digits = sum(1 for a, b in zip(gt, recognized_text) if a == b)\n",
    "    ocr_accuracy = correct_digits / len(gt) if len(gt) > 0 else 0.0\n",
    "    ocr_binary = 1.0 if recognized_text == gt else 0.0\n",
    "    return recognized_text, ocr_accuracy, ocr_binary\n",
    "\n",
    "# --------------------------------------\n",
    "# Compute metrics for each (alpha, beta)\n",
    "# --------------------------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "\n",
    "psnr_dict_worst = {}\n",
    "ssim_dict_worst = {}\n",
    "ocr_acc_dict_avg = {}\n",
    "ocr_bin_dict_avg = {}\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "for meta_file in tqdm(metadata_files, desc=\"Processing images\", unit=\"image\"):\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    alpha, beta = metadata['alpha'], metadata['beta']\n",
    "    digit_bboxes = metadata['digit_bboxes']\n",
    "    plate_number_gt = metadata['plate_number']\n",
    "\n",
    "    idx = metadata['idx']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    if not (os.path.exists(original_path) and os.path.exists(distorted_path)):\n",
    "        continue\n",
    "\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_img = model(distorted_img)\n",
    "        generated_img = torch.clamp(generated_img, 0.0, 1.0)\n",
    "\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    generated_np = generated_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, generated_np, digit_bboxes)\n",
    "\n",
    "    # Take the worst (minimum) PSNR and SSIM values across all digits for this image\n",
    "    worst_psnr = np.min(psnr_per_number) if psnr_per_number else 0.0\n",
    "    worst_ssim = np.min(ssim_per_number) if ssim_per_number else 0.0\n",
    "\n",
    "    image_bgr = (generated_np * 255).astype(np.uint8)\n",
    "    image_bgr = cv2.cvtColor(image_bgr, cv2.COLOR_RGB2BGR)\n",
    "    recognized_text, ocr_accuracy, ocr_binary = compute_ocr_metrics(image_bgr, updated_bboxes, plate_number_gt, margin = 2)\n",
    "\n",
    "    if (alpha, beta) not in psnr_dict_worst:\n",
    "        psnr_dict_worst[(alpha, beta)] = []\n",
    "        ssim_dict_worst[(alpha, beta)] = []\n",
    "        ocr_acc_dict_avg[(alpha, beta)] = []\n",
    "        ocr_bin_dict_avg[(alpha, beta)] = []\n",
    "\n",
    "    psnr_dict_worst[(alpha, beta)].append(worst_psnr)\n",
    "    ssim_dict_worst[(alpha, beta)].append(worst_ssim)\n",
    "    ocr_acc_dict_avg[(alpha, beta)].append(ocr_accuracy)\n",
    "    ocr_bin_dict_avg[(alpha, beta)].append(ocr_binary)\n",
    "\n",
    "alpha_values = sorted(set(a for (a, b) in psnr_dict_worst.keys()))\n",
    "beta_values = sorted(set(b for (a, b) in psnr_dict_worst.keys()))\n",
    "num_alphas, num_betas = len(alpha_values), len(beta_values)\n",
    "\n",
    "def create_matrix_from_dict(data_dict):\n",
    "    mat = np.full((num_betas, num_alphas), np.nan)\n",
    "    alpha_to_idx = {val: i for i, val in enumerate(alpha_values)}\n",
    "    beta_to_idx = {val: i for i, val in enumerate(beta_values)}\n",
    "    for (a, b), val_list  in data_dict.items():\n",
    "        val = np.min(val_list) if val_list else np.nan\n",
    "        mat[beta_to_idx[b], alpha_to_idx[a]] = val\n",
    "    return mat\n",
    "\n",
    "psnr_matrix = create_matrix_from_dict(psnr_dict_worst)\n",
    "ssim_matrix = create_matrix_from_dict(ssim_dict_worst)\n",
    "ocr_acc_matrix = create_matrix_from_dict(ocr_acc_dict_avg)\n",
    "ocr_bin_matrix = create_matrix_from_dict(ocr_bin_dict_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Interactive Plot with Buttons\n",
    "# -----------------------------------\n",
    "current_metric = 'PSNR'\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "plt.subplots_adjust(bottom=0.2)  # space for buttons\n",
    "\n",
    "im = ax.imshow(psnr_matrix, origin='lower', aspect='auto', cmap=\"viridis\")\n",
    "ax.set_title(\"Worst PSNR per Image (Minimum Digit PSNR)\")\n",
    "cb = plt.colorbar(im, ax=ax, label='PSNR (dB)')\n",
    "ax.set_xticks(range(0, num_alphas, 5))\n",
    "ax.set_xticklabels(alpha_values[::5])\n",
    "ax.set_yticks(range(0, num_betas, 5))\n",
    "ax.set_yticklabels(beta_values[::5])\n",
    "ax.set_xlabel(\"Alpha (degrees)\")\n",
    "ax.set_ylabel(\"Beta (degrees)\")\n",
    "\n",
    "def format_coord(x, y):\n",
    "    col = int(round(x))\n",
    "    row = int(round(y))\n",
    "    if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "        alpha = alpha_values[col]\n",
    "        beta = beta_values[row]\n",
    "        if current_metric == 'PSNR':\n",
    "            val = psnr_matrix[row, col]\n",
    "            return f\"Alpha: {alpha}, Beta: {beta}, Worst PSNR: {val:.2f} dB\" if not np.isnan(val) else \"N/A\"\n",
    "        elif current_metric == 'SSIM':\n",
    "            val = ssim_matrix[row, col]\n",
    "            return f\"Alpha: {alpha}, Beta: {beta}, Worst SSIM: {val:.3f}\" if not np.isnan(val) else \"N/A\"\n",
    "        elif current_metric == 'OCR_Accuracy':\n",
    "            val = ocr_acc_matrix[row, col]\n",
    "            return f\"Alpha: {alpha}, Beta: {beta}, OCR Acc: {val*100:.2f}%\" if not np.isnan(val) else \"N/A\"\n",
    "        elif current_metric == 'OCR_Binary':\n",
    "            val = ocr_bin_matrix[row, col]\n",
    "            return f\"Alpha: {alpha}, Beta: {beta}, OCR Binary: {val:.0f}\" if not np.isnan(val) else \"N/A\"\n",
    "    return \"N/A\"\n",
    "\n",
    "ax.format_coord = format_coord\n",
    "\n",
    "def show_image_details_for(alpha, beta):\n",
    "    # Re-run the detailed view logic\n",
    "    found_file = None\n",
    "    for meta_file in metadata_files:\n",
    "        meta_path = os.path.join(data_dir, meta_file)\n",
    "        with open(meta_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        if metadata['alpha'] == alpha and metadata['beta'] == beta:\n",
    "            found_file = metadata\n",
    "            break\n",
    "\n",
    "    if found_file is None:\n",
    "        print(\"No images found for that angle.\")\n",
    "        return\n",
    "\n",
    "    found_file['digit_bboxes'].sort(key=lambda bbox: bbox[0])\n",
    "    idx = found_file['idx']\n",
    "    plate_number_gt = found_file['plate_number']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_tensor = model(distorted_img)\n",
    "        generated_tensor = torch.clamp(generated_tensor, 0.0, 1.0)\n",
    "\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    generated_np = generated_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    psnr_vals, ssim_vals, updated_bboxes = align_and_update_bboxes(original_np, generated_np, found_file['digit_bboxes'])\n",
    "\n",
    "    generated_bgr = (generated_np*255).astype(np.uint8)\n",
    "    generated_bgr = cv2.cvtColor(generated_bgr, cv2.COLOR_RGB2BGR)\n",
    "    recognized_text, ocr_accuracy, ocr_binary = compute_ocr_metrics(generated_bgr, updated_bboxes, plate_number_gt, margin = 2)\n",
    "\n",
    "    original_image_cv = cv2.imread(original_path)\n",
    "    for i, bbox in enumerate(found_file['digit_bboxes'], start=1):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0,0,255),1)\n",
    "        cv2.putText(original_image_cv, str(i), (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,150,0),1)\n",
    "    original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    generated_show = generated_bgr.copy()\n",
    "    for i,bbox in enumerate(updated_bboxes, start=1):\n",
    "        x,y,w,h = bbox\n",
    "        cv2.rectangle(generated_show, (x,y),(x+w,y+h),(0,0,255),1)\n",
    "        cv2.putText(generated_show,str(i),(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,150,0),1)\n",
    "    generated_image_rgb = cv2.cvtColor(generated_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    table_data = [[\"Digit\",\"PSNR(dB)\",\"SSIM\"]]\n",
    "    for i,(p,s) in enumerate(zip(psnr_vals, ssim_vals), start=1):\n",
    "        table_data.append([str(i), f\"{p:.2f}\", f\"{s:.3f}\"])\n",
    "    transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "    fig2 = plt.figure(figsize=(14,7))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(original_image_rgb)\n",
    "    plt.title(f'Original Image (Alpha={alpha}, Beta={beta})')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(generated_image_rgb)\n",
    "    plt.title(f'Generated Image\\nGT: {plate_number_gt}, Rec: {recognized_text}, OCR Acc: {ocr_accuracy*100:.2f}%, Binary: {ocr_binary}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    table = plt.table(cellText=transposed_table_data,\n",
    "                      cellLoc='center',\n",
    "                      loc='center',\n",
    "                      bbox=[0,-0.55,1,0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def on_click(event):\n",
    "    if event.inaxes == ax:\n",
    "        x, y = event.xdata, event.ydata\n",
    "        if x is None or y is None:\n",
    "            return\n",
    "        col = int(round(x))\n",
    "        row = int(round(y))\n",
    "        if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "            alpha = alpha_values[col]\n",
    "            beta = beta_values[row]\n",
    "            show_image_details_for(alpha, beta)\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "\n",
    "# Add buttons to switch between metrics\n",
    "ax_psnr = plt.axes([0.1, 0.05, 0.1, 0.05])\n",
    "ax_ssim = plt.axes([0.22, 0.05, 0.1, 0.05])\n",
    "ax_ocr_acc = plt.axes([0.34, 0.05, 0.12, 0.05])\n",
    "ax_ocr_bin = plt.axes([0.48, 0.05, 0.1, 0.05])\n",
    "\n",
    "btn_psnr = Button(ax_psnr, 'PSNR')\n",
    "btn_ssim = Button(ax_ssim, 'SSIM')\n",
    "btn_ocr_acc = Button(ax_ocr_acc, 'OCR Acc')\n",
    "btn_ocr_bin = Button(ax_ocr_bin, 'OCR Bin')\n",
    "\n",
    "def update_heatmap(metric):\n",
    "    global current_metric\n",
    "    current_metric = metric\n",
    "    ax.clear()\n",
    "    if metric == 'PSNR':\n",
    "        data = psnr_matrix\n",
    "        title = \"Worst PSNR per Image (Minimum Digit PSNR)\"\n",
    "        cbar_label = \"PSNR (dB)\"\n",
    "    elif metric == 'SSIM':\n",
    "        data = ssim_matrix\n",
    "        title = \"Worst SSIM per Image (Minimum Digit SSIM)\"\n",
    "        cbar_label = \"SSIM\"\n",
    "    elif metric == 'OCR_Accuracy':\n",
    "        data = ocr_acc_matrix\n",
    "        title = \"Average OCR Accuracy\"\n",
    "        cbar_label = \"OCR Acc\"\n",
    "    else:\n",
    "        data = ocr_bin_matrix\n",
    "        title = \"OCR Binary (1=All Correct)\"\n",
    "        cbar_label = \"OCR Binary\"\n",
    "\n",
    "    im = ax.imshow(data, origin='lower', aspect='auto', cmap='viridis')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(0,num_alphas,5))\n",
    "    ax.set_xticklabels(alpha_values[::5])\n",
    "    ax.set_yticks(range(0,num_betas,5))\n",
    "    ax.set_yticklabels(beta_values[::5])\n",
    "    ax.set_xlabel(\"Alpha (degrees)\")\n",
    "    ax.set_ylabel(\"Beta (degrees)\")\n",
    "    ax.format_coord = format_coord\n",
    "    fig.colorbar(im, ax=ax, label=cbar_label)\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "def on_psnr_clicked(event):\n",
    "    update_heatmap('PSNR')\n",
    "\n",
    "def on_ssim_clicked(event):\n",
    "    update_heatmap('SSIM')\n",
    "\n",
    "def on_ocr_acc_clicked(event):\n",
    "    update_heatmap('OCR_Accuracy')\n",
    "\n",
    "def on_ocr_bin_clicked(event):\n",
    "    update_heatmap('OCR_Binary')\n",
    "\n",
    "btn_psnr.on_clicked(on_psnr_clicked)\n",
    "btn_ssim.on_clicked(on_ssim_clicked)\n",
    "btn_ocr_acc.on_clicked(on_ocr_acc_clicked)\n",
    "btn_ocr_bin.on_clicked(on_ocr_bin_clicked)\n",
    "\n",
    "current_metric = 'PSNR'  # default\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from run af175db889874ff596e0a404c5c26901 in experiment 'Unet' successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 8100/8100 [1:11:44<00:00,  1.88image/s]\n"
     ]
    }
   ],
   "source": [
    "# #Heatmap of worst PSNR, SSIM, and OCR in parallel\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import cv2\n",
    "from pytorch_msssim import ssim\n",
    "import pytesseract\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# --------------------\n",
    "# Configuration\n",
    "# --------------------\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# MLflow model load\n",
    "mlflow.set_experiment('Unet')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet')\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval().to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "# --------------------\n",
    "# Functions \n",
    "# --------------------\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "def ocr_single_digit(image_bgr):\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "    config = r'--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789'\n",
    "    text = pytesseract.image_to_string(thresh, config=config).strip()\n",
    "    if len(text) == 1 and text.isdigit():\n",
    "        return text\n",
    "    return '?'\n",
    "\n",
    "def align_and_update_bboxes(original_np, generated_np, digit_bboxes):\n",
    "    search_margin = 16\n",
    "\n",
    "    def process_digit_bbox(bbox):\n",
    "        x, y, w, h = bbox\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(generated_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(generated_np.shape[0], y + h + search_margin)\n",
    "        search_region = generated_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        # Compute PSNR and SSIM\n",
    "        aligned_digit = generated_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2,0,1)).unsqueeze(0)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2,0,1)).unsqueeze(0)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "\n",
    "        return psnr_val, ssim_val, (best_x, best_y, w, h)\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_digit_bbox)(bbox) for bbox in digit_bboxes)\n",
    "    psnr_values = [r[0] for r in results]\n",
    "    ssim_values = [r[1] for r in results]\n",
    "    updated_bboxes = [r[2] for r in results]\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "def compute_ocr_metrics(image_bgr, updated_bboxes, plate_number_gt, margin):\n",
    "    def process_bbox(bbox):\n",
    "        x, y, w, h = bbox\n",
    "        x1 = max(0, x - margin)\n",
    "        y1 = max(0, y - margin)\n",
    "        x2 = min(image_bgr.shape[1], x + w + margin)\n",
    "        y2 = min(image_bgr.shape[0], y + h + margin)\n",
    "        digit_patch = image_bgr[y1:y2, x1:x2]\n",
    "        recognized_digit = ocr_single_digit(digit_patch)\n",
    "        return recognized_digit\n",
    "\n",
    "    recognized_digits = Parallel(n_jobs=-1)(delayed(process_bbox)(bbox) for bbox in updated_bboxes)\n",
    "    recognized_text = \"\".join(recognized_digits)\n",
    "    gt = plate_number_gt\n",
    "    correct_digits = sum(1 for a, b in zip(gt, recognized_text) if a == b)\n",
    "    ocr_accuracy = correct_digits / len(gt) if len(gt) > 0 else 0.0\n",
    "    ocr_binary = 1.0 if recognized_text == gt else 0.0\n",
    "    return recognized_text, ocr_accuracy, ocr_binary\n",
    "\n",
    "# --------------------------------------\n",
    "# Compute metrics for each (alpha, beta)\n",
    "# --------------------------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "\n",
    "psnr_dict_worst = {}\n",
    "ssim_dict_worst = {}\n",
    "ocr_acc_dict_avg = {}\n",
    "ocr_bin_dict_avg = {}\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "for meta_file in tqdm(metadata_files, desc=\"Processing images\", unit=\"image\"):\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    alpha, beta = metadata['alpha'], metadata['beta']\n",
    "    digit_bboxes = metadata['digit_bboxes']\n",
    "    plate_number_gt = metadata['plate_number']\n",
    "\n",
    "    idx = metadata['idx']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    if not (os.path.exists(original_path) and os.path.exists(distorted_path)):\n",
    "        continue\n",
    "\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_img = model(distorted_img)\n",
    "        generated_img = torch.clamp(generated_img, 0.0, 1.0)\n",
    "\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    generated_np = generated_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    # Parallelized CPU operations\n",
    "    psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, generated_np, digit_bboxes)\n",
    "    image_bgr = (generated_np * 255).astype(np.uint8)\n",
    "    image_bgr = cv2.cvtColor(image_bgr, cv2.COLOR_RGB2BGR)\n",
    "    recognized_text, ocr_accuracy, ocr_binary = compute_ocr_metrics(image_bgr, updated_bboxes, plate_number_gt, margin=2)\n",
    "\n",
    "    # Take worst PSNR and SSIM\n",
    "    worst_psnr = np.min(psnr_per_number) if psnr_per_number else 0.0\n",
    "    worst_ssim = np.min(ssim_per_number) if ssim_per_number else 0.0\n",
    "\n",
    "    if (alpha, beta) not in psnr_dict_worst:\n",
    "        psnr_dict_worst[(alpha, beta)] = []\n",
    "        ssim_dict_worst[(alpha, beta)] = []\n",
    "        ocr_acc_dict_avg[(alpha, beta)] = []\n",
    "        ocr_bin_dict_avg[(alpha, beta)] = []\n",
    "\n",
    "    psnr_dict_worst[(alpha, beta)].append(worst_psnr)\n",
    "    ssim_dict_worst[(alpha, beta)].append(worst_ssim)\n",
    "    ocr_acc_dict_avg[(alpha, beta)].append(ocr_accuracy)\n",
    "    ocr_bin_dict_avg[(alpha, beta)].append(ocr_binary)\n",
    "\n",
    "alpha_values = sorted(set(a for (a, b) in psnr_dict_worst.keys()))\n",
    "beta_values = sorted(set(b for (a, b) in psnr_dict_worst.keys()))\n",
    "num_alphas, num_betas = len(alpha_values), len(beta_values)\n",
    "\n",
    "def create_matrix_from_dict(data_dict):\n",
    "    mat = np.full((num_betas, num_alphas), np.nan)\n",
    "    alpha_to_idx = {val: i for i, val in enumerate(alpha_values)}\n",
    "    beta_to_idx = {val: i for i, val in enumerate(beta_values)}\n",
    "    for (a, b), val_list  in data_dict.items():\n",
    "        val = np.min(val_list) if val_list else np.nan\n",
    "        mat[beta_to_idx[b], alpha_to_idx[a]] = val\n",
    "    return mat\n",
    "\n",
    "psnr_matrix = create_matrix_from_dict(psnr_dict_worst)\n",
    "ssim_matrix = create_matrix_from_dict(ssim_dict_worst)\n",
    "ocr_acc_matrix = create_matrix_from_dict(ocr_acc_dict_avg)\n",
    "ocr_bin_matrix = create_matrix_from_dict(ocr_bin_dict_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_details_for(alpha, beta):\n",
    "    # Re-run the detailed view logic\n",
    "    found_file = None\n",
    "    for meta_file in metadata_files:\n",
    "        meta_path = os.path.join(data_dir, meta_file)\n",
    "        with open(meta_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        if metadata['alpha'] == alpha and metadata['beta'] == beta:\n",
    "            found_file = metadata\n",
    "            break\n",
    "\n",
    "    if found_file is None:\n",
    "        print(\"No images found for that angle.\")\n",
    "        return\n",
    "\n",
    "    found_file['digit_bboxes'].sort(key=lambda bbox: bbox[0])\n",
    "    idx = found_file['idx']\n",
    "    plate_number_gt = found_file['plate_number']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_tensor = model(distorted_img)\n",
    "        generated_tensor = torch.clamp(generated_tensor, 0.0, 1.0)\n",
    "\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    generated_np = generated_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    psnr_vals, ssim_vals, updated_bboxes = align_and_update_bboxes(original_np, generated_np, found_file['digit_bboxes'])\n",
    "\n",
    "    generated_bgr = (generated_np*255).astype(np.uint8)\n",
    "    generated_bgr = cv2.cvtColor(generated_bgr, cv2.COLOR_RGB2BGR)\n",
    "    recognized_text, ocr_accuracy, ocr_binary = compute_ocr_metrics(generated_bgr, updated_bboxes, plate_number_gt, margin = 2)\n",
    "\n",
    "    original_image_cv = cv2.imread(original_path)\n",
    "    for i, bbox in enumerate(found_file['digit_bboxes'], start=1):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0,0,255),1)\n",
    "        cv2.putText(original_image_cv, str(i), (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,150,0),1)\n",
    "    original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    generated_show = generated_bgr.copy()\n",
    "    for i,bbox in enumerate(updated_bboxes, start=1):\n",
    "        x,y,w,h = bbox\n",
    "        cv2.rectangle(generated_show, (x,y),(x+w,y+h),(0,0,255),1)\n",
    "        cv2.putText(generated_show,str(i),(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,150,0),1)\n",
    "    generated_image_rgb = cv2.cvtColor(generated_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    table_data = [[\"Digit\",\"PSNR(dB)\",\"SSIM\"]]\n",
    "    for i,(p,s) in enumerate(zip(psnr_vals, ssim_vals), start=1):\n",
    "        table_data.append([str(i), f\"{p:.2f}\", f\"{s:.3f}\"])\n",
    "    transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "    fig2 = plt.figure(figsize=(14,7))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(original_image_rgb)\n",
    "    plt.title(f'Original Image (Alpha={alpha}, Beta={beta})')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(generated_image_rgb)\n",
    "    plt.title(f'Generated Image\\nGT: {plate_number_gt}, Rec: {recognized_text}, OCR Acc: {ocr_accuracy*100:.2f}%, Binary: {ocr_binary}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    table = plt.table(cellText=transposed_table_data,\n",
    "                      cellLoc='center',\n",
    "                      loc='center',\n",
    "                      bbox=[0,-0.55,1,0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "psnr_matrix_clipped = np.clip(psnr_matrix, None, 20)\n",
    "\n",
    "current_metric = 'PSNR'\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "plt.subplots_adjust(bottom=0.15)  # space for buttons\n",
    "\n",
    "# Draw initial heatmap\n",
    "im = ax.imshow(psnr_matrix_clipped, origin='lower', aspect='auto', cmap=\"viridis\")\n",
    "ax.set_title(\"Worst PSNR per Image (Minimum Digit PSNR)\")\n",
    "cbar = plt.colorbar(im, ax=ax, label='PSNR (dB)')\n",
    "ax.set_xticks(range(0, num_alphas, 5))\n",
    "ax.set_xticklabels(alpha_values[::5])\n",
    "ax.set_yticks(range(0, num_betas, 5))\n",
    "ax.set_yticklabels(beta_values[::5])\n",
    "ax.set_xlabel(\"Alpha (degrees)\")\n",
    "ax.set_ylabel(\"Beta (degrees)\")\n",
    "ax.format_coord = format_coord  # Set the coordinate display format\n",
    "\n",
    "# Define button positions\n",
    "button_width = 0.1   # Button width\n",
    "button_height = 0.05  # Button height\n",
    "button_spacing = 0.02  # Space between buttons\n",
    "\n",
    "# Compute x-coordinates for buttons\n",
    "x_start = 0.2  # Starting x-position\n",
    "y_position = 0.03\n",
    "x_psnr = x_start\n",
    "x_ssim = x_psnr + button_width + button_spacing\n",
    "x_ocr_acc = x_ssim + button_width + button_spacing\n",
    "x_ocr_bin = x_ocr_acc + button_width + button_spacing\n",
    "\n",
    "# Add buttons\n",
    "ax_psnr = plt.axes([x_psnr, y_position, button_width, button_height])\n",
    "ax_ssim = plt.axes([x_ssim, y_position, button_width, button_height])\n",
    "ax_ocr_acc = plt.axes([x_ocr_acc, y_position, button_width, button_height])\n",
    "ax_ocr_bin = plt.axes([x_ocr_bin, y_position, button_width, button_height])\n",
    "\n",
    "btn_psnr = Button(ax_psnr, 'PSNR')\n",
    "btn_ssim = Button(ax_ssim, 'SSIM')\n",
    "btn_ocr_acc = Button(ax_ocr_acc, 'OCR Acc')\n",
    "btn_ocr_bin = Button(ax_ocr_bin, 'OCR Bin')\n",
    "\n",
    "def update_heatmap(metric):\n",
    "    global current_metric\n",
    "    current_metric = metric\n",
    "    ax.clear()\n",
    "    \n",
    "    if metric == 'PSNR':\n",
    "        data = psnr_matrix_clipped\n",
    "        title = \"Worst PSNR per Image (Minimum Digit PSNR)\"\n",
    "        cbar_label = \"PSNR (dB)\"\n",
    "    elif metric == 'SSIM':\n",
    "        data = ssim_matrix\n",
    "        title = \"Worst SSIM per Image (Minimum Digit SSIM)\"\n",
    "        cbar_label = \"SSIM\"\n",
    "    elif metric == 'OCR_Accuracy':\n",
    "        data = ocr_acc_matrix\n",
    "        title = \"Average OCR Accuracy\"\n",
    "        cbar_label = \"OCR Acc\"\n",
    "    else:\n",
    "        data = ocr_bin_matrix\n",
    "        title = \"OCR Binary (1=All Correct)\"\n",
    "        cbar_label = \"OCR Binary\"\n",
    "\n",
    "    # Update heatmap\n",
    "    im = ax.imshow(data, origin='lower', aspect='auto', cmap='viridis')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(0, num_alphas, 5))\n",
    "    ax.set_xticklabels(alpha_values[::5])\n",
    "    ax.set_yticks(range(0, num_betas, 5))\n",
    "    ax.set_yticklabels(beta_values[::5])\n",
    "    ax.set_xlabel(\"Alpha (degrees)\")\n",
    "    ax.set_ylabel(\"Beta (degrees)\")\n",
    "\n",
    "    # Update colorbar\n",
    "    cbar.mappable = im\n",
    "    cbar.set_label(cbar_label)\n",
    "    cbar.update_normal(im)\n",
    "\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "def on_psnr_clicked(event):\n",
    "    update_heatmap('PSNR')\n",
    "\n",
    "def on_ssim_clicked(event):\n",
    "    update_heatmap('SSIM')\n",
    "\n",
    "def on_ocr_acc_clicked(event):\n",
    "    update_heatmap('OCR_Accuracy')\n",
    "\n",
    "def on_ocr_bin_clicked(event):\n",
    "    update_heatmap('OCR_Binary')\n",
    "\n",
    "btn_psnr.on_clicked(on_psnr_clicked)\n",
    "btn_ssim.on_clicked(on_ssim_clicked)\n",
    "btn_ocr_acc.on_clicked(on_ocr_acc_clicked)\n",
    "btn_ocr_bin.on_clicked(on_ocr_bin_clicked)\n",
    "\n",
    "# Connect the click event after setting up the entire figure\n",
    "def on_click(event):\n",
    "    if event.inaxes == ax:  # Ensure the click is within the heatmap axis\n",
    "        x, y = event.xdata, event.ydata\n",
    "        if x is None or y is None:\n",
    "            return\n",
    "        col = int(round(x))\n",
    "        row = int(round(y))\n",
    "        if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "            alpha = alpha_values[col]\n",
    "            beta = beta_values[row]\n",
    "            show_image_details_for(alpha, beta)\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', on_click)  # Connect after all setups\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def show_image_details_for(alpha, beta):\n",
    "    # Re-run the detailed view logic\n",
    "    found_file = None\n",
    "    for meta_file in metadata_files:\n",
    "        meta_path = os.path.join(data_dir, meta_file)\n",
    "        with open(meta_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        if metadata['alpha'] == alpha and metadata['beta'] == beta:\n",
    "            found_file = metadata\n",
    "            break\n",
    "\n",
    "    if found_file is None:\n",
    "        print(\"No images found for that angle.\")\n",
    "        return\n",
    "\n",
    "    found_file['digit_bboxes'].sort(key=lambda bbox: bbox[0])\n",
    "    idx = found_file['idx']\n",
    "    plate_number_gt = found_file['plate_number']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    # Load images as tensors for generation\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_tensor = model(distorted_img)\n",
    "        generated_tensor = torch.clamp(generated_tensor, 0.0, 1.0)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    generated_np = generated_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    # Compute PSNR, SSIM, and bounding boxes\n",
    "    psnr_vals, ssim_vals, updated_bboxes = align_and_update_bboxes(original_np, generated_np, found_file['digit_bboxes'])\n",
    "\n",
    "    # Prepare images for display\n",
    "    distorted_image_cv = cv2.imread(distorted_path)\n",
    "    distorted_image_rgb = cv2.cvtColor(distorted_image_cv, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Original image (with rectangles and text)\n",
    "    original_image_cv = cv2.imread(original_path)\n",
    "    for i, bbox in enumerate(found_file['digit_bboxes'], start=1):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0,0,255),1)\n",
    "        cv2.putText(original_image_cv, str(i), (x,y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,150,0),1)\n",
    "    original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Generated image (with rectangles and text)\n",
    "    generated_bgr = (generated_np * 255).astype(np.uint8)\n",
    "    generated_bgr = cv2.cvtColor(generated_bgr, cv2.COLOR_RGB2BGR)  \n",
    "    generated_show = generated_bgr.copy()\n",
    "    for i,bbox in enumerate(updated_bboxes, start=1):\n",
    "        x,y,w,h = bbox\n",
    "        cv2.rectangle(generated_show, (x,y),(x+w,y+h),(0,0,255),1)\n",
    "        cv2.putText(generated_show,str(i),(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,150,0),1)\n",
    "    generated_image_rgb = cv2.cvtColor(generated_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    recognized_text, ocr_accuracy, ocr_binary = compute_ocr_metrics(generated_bgr, updated_bboxes, plate_number_gt, margin=2)\n",
    "\n",
    "    # Prepare table data\n",
    "    table_data = [[\"Digit\",\"PSNR(dB)\",\"SSIM\"]]\n",
    "    for i,(p,s) in enumerate(zip(psnr_vals, ssim_vals), start=1):\n",
    "        table_data.append([str(i), f\"{p:.2f}\", f\"{s:.3f}\"])\n",
    "    transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "    # Create figure with 3 rows: Distorted, Original, Generated\n",
    "    fig2 = plt.figure(figsize=(11,9))\n",
    "\n",
    "    # Distorted image \n",
    "    plt.subplot(3,1,1)\n",
    "    plt.imshow(distorted_image_rgb)\n",
    "    plt.title(f'Distorted Image (Alpha={alpha}, Beta={beta})')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Original image \n",
    "    plt.subplot(3,1,2)\n",
    "    plt.imshow(original_image_rgb)\n",
    "    plt.title(f'Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Generated image \n",
    "    plt.subplot(3,1,3)\n",
    "    plt.imshow(generated_image_rgb)\n",
    "    plt.title(f'Generated Image\\nGT: {plate_number_gt}, Rec: {recognized_text}, OCR Acc: {ocr_accuracy*100:.2f}%, Binary: {int(ocr_binary)}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Add table below all images\n",
    "    # Adjust the bbox so it appears below the last subplot\n",
    "    table = plt.table(cellText=transposed_table_data,\n",
    "                      cellLoc='center',\n",
    "                      loc='center',\n",
    "                      bbox=[0,-0.5, 1,0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def format_coord(x, y):\n",
    "    col = int(round(x))\n",
    "    row = int(round(y))\n",
    "    if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "        alpha = alpha_values[col]\n",
    "        beta = beta_values[row]\n",
    "        psnr_value = psnr_matrix_clipped[row, col]\n",
    "        return f\"Alpha: {alpha:.0f}, Beta: {beta:.0f}, PSNR: {psnr_value:.2f} dB\" if not np.isnan(psnr_value) else f\"Alpha: {alpha:.0f}, Beta: {beta:.0f}, PSNR: N/A\"\n",
    "    return \"Alpha: N/A, Beta: N/A\"\n",
    "\n",
    "psnr_matrix_clipped = np.clip(psnr_matrix, None, 20)\n",
    "\n",
    "current_metric = 'PSNR'\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "plt.subplots_adjust(bottom=0.15)  # space for buttons\n",
    "\n",
    "# Draw initial heatmap\n",
    "im = ax.imshow(psnr_matrix_clipped, origin='lower', aspect='auto', cmap=\"viridis\")\n",
    "ax.set_title(\"Worst PSNR per Image (Minimum Digit PSNR)\")\n",
    "cbar = plt.colorbar(im, ax=ax, label='PSNR (dB)')\n",
    "ax.set_xticks(range(0, num_alphas, 5))\n",
    "ax.set_xticklabels(alpha_values[::5])\n",
    "ax.set_yticks(range(0, num_betas, 5))\n",
    "ax.set_yticklabels(beta_values[::5])\n",
    "ax.set_xlabel(\"Alpha (degrees)\")\n",
    "ax.set_ylabel(\"Beta (degrees)\")\n",
    "ax.format_coord = format_coord  # Set the coordinate display format\n",
    "\n",
    "# Define button positions\n",
    "button_width = 0.1   # Button width\n",
    "button_height = 0.05  # Button height\n",
    "button_spacing = 0.02  # Space between buttons\n",
    "\n",
    "# Compute x-coordinates for buttons\n",
    "x_start = 0.2  # Starting x-position\n",
    "y_position = 0.03\n",
    "x_psnr = x_start\n",
    "x_ssim = x_psnr + button_width + button_spacing\n",
    "x_ocr_acc = x_ssim + button_width + button_spacing\n",
    "x_ocr_bin = x_ocr_acc + button_width + button_spacing\n",
    "\n",
    "# Add buttons\n",
    "ax_psnr = plt.axes([x_psnr, y_position, button_width, button_height])\n",
    "ax_ssim = plt.axes([x_ssim, y_position, button_width, button_height])\n",
    "ax_ocr_acc = plt.axes([x_ocr_acc, y_position, button_width, button_height])\n",
    "ax_ocr_bin = plt.axes([x_ocr_bin, y_position, button_width, button_height])\n",
    "\n",
    "btn_psnr = Button(ax_psnr, 'PSNR')\n",
    "btn_ssim = Button(ax_ssim, 'SSIM')\n",
    "btn_ocr_acc = Button(ax_ocr_acc, 'OCR Acc')\n",
    "btn_ocr_bin = Button(ax_ocr_bin, 'OCR Bin')\n",
    "\n",
    "def update_heatmap(metric):\n",
    "    global current_metric\n",
    "    current_metric = metric\n",
    "    ax.clear()\n",
    "    \n",
    "    if metric == 'PSNR':\n",
    "        data = psnr_matrix_clipped\n",
    "        title = \"Worst PSNR per Image (Minimum Digit PSNR)\"\n",
    "        cbar_label = \"PSNR (dB)\"\n",
    "    elif metric == 'SSIM':\n",
    "        data = ssim_matrix\n",
    "        title = \"Worst SSIM per Image (Minimum Digit SSIM)\"\n",
    "        cbar_label = \"SSIM\"\n",
    "    elif metric == 'OCR_Accuracy':\n",
    "        data = ocr_acc_matrix\n",
    "        title = \"OCR Accuracy\"\n",
    "        cbar_label = \"OCR Acc\"\n",
    "    else:\n",
    "        data = ocr_bin_matrix\n",
    "        title = \"OCR Binary Accuracy (1=All Correct)\"\n",
    "        cbar_label = \"OCR Binary\"\n",
    "\n",
    "    # Update heatmap\n",
    "    im = ax.imshow(data, origin='lower', aspect='auto', cmap='viridis')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(0, num_alphas, 5))\n",
    "    ax.set_xticklabels(alpha_values[::5])\n",
    "    ax.set_yticks(range(0, num_betas, 5))\n",
    "    ax.set_yticklabels(beta_values[::5])\n",
    "    ax.set_xlabel(\"Alpha (degrees)\")\n",
    "    ax.set_ylabel(\"Beta (degrees)\")\n",
    "\n",
    "    # Update colorbar\n",
    "    cbar.mappable = im\n",
    "    cbar.set_label(cbar_label)\n",
    "    cbar.update_normal(im)\n",
    "\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "def on_psnr_clicked(event):\n",
    "    update_heatmap('PSNR')\n",
    "\n",
    "def on_ssim_clicked(event):\n",
    "    update_heatmap('SSIM')\n",
    "\n",
    "def on_ocr_acc_clicked(event):\n",
    "    update_heatmap('OCR_Accuracy')\n",
    "\n",
    "def on_ocr_bin_clicked(event):\n",
    "    update_heatmap('OCR_Binary')\n",
    "\n",
    "btn_psnr.on_clicked(on_psnr_clicked)\n",
    "btn_ssim.on_clicked(on_ssim_clicked)\n",
    "btn_ocr_acc.on_clicked(on_ocr_acc_clicked)\n",
    "btn_ocr_bin.on_clicked(on_ocr_bin_clicked)\n",
    "\n",
    "# Connect the click event after setting up the entire figure\n",
    "def on_click(event):\n",
    "    if event.inaxes == ax:  # Ensure the click is within the heatmap axis\n",
    "        x, y = event.xdata, event.ydata\n",
    "        if x is None or y is None:\n",
    "            return\n",
    "        col = int(round(x))\n",
    "        row = int(round(y))\n",
    "        if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "            alpha = alpha_values[col]\n",
    "            beta = beta_values[row]\n",
    "            show_image_details_for(alpha, beta)\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', on_click)  # Connect after all setups\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
