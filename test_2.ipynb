{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Digit Alignment Using Template Matching\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from PIL import Image\n",
    "\n",
    "# Target alpha and beta values\n",
    "target_alpha = 88\n",
    "target_beta = 2\n",
    "\n",
    "# Directory containing metadata files and images\n",
    "data_dir = \"data_test\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set the MLflow experiment and load the model\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "# Define PSNR calculation\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "# Transform to convert PIL image to tensor in [0,1]\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Scan metadata files\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "found_file = None\n",
    "\n",
    "# Find matching alpha and beta\n",
    "for meta_file in metadata_files:\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "        found_file = {\n",
    "            \"metadata_file\": meta_file,\n",
    "            \"index\": metadata.get('idx'),\n",
    "            \"noise_level\": metadata.get('noise_level'),\n",
    "            \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "            \"plate_number\": metadata.get('plate_number')\n",
    "        }\n",
    "        break\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "print(f\"Noise Level: {found_file['noise_level']:.2f}\")\n",
    "print(f\"Plate Number: {found_file['plate_number']}\")\n",
    "\n",
    "# Sort bounding boxes\n",
    "original_bboxes = sorted(found_file['digit_bboxes'], key=lambda bbox: bbox[0])\n",
    "\n",
    "# Load images\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "# Convert to NumPy\n",
    "original_np = original_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Align and update bounding boxes for the reconstructed image\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Extract original digit\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        # Extract aligned digit\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Compute metrics\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "# Perform alignment\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "# Visualization\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "# Draw original bounding boxes\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Prepare table\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title('Reconstructed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best PSNR Selection with Template Matching\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import sys\n",
    "import cv2\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# -----------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------\n",
    "data_dir = \"data_test\"\n",
    "moderate_noise_threshold = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -----------------------------------\n",
    "# Load Model\n",
    "# -----------------------------------\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "runs = client.search_runs(experiment_ids=experiment.experiment_id, order_by=[\"attributes.start_time DESC\"])\n",
    "run_id = runs[0].info.run_id  # Get the last run\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment {experiment.name}\")\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    \"\"\"Calculate PSNR between two [0,1] tensor images using PyTorch functions.\"\"\"\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Function to align and update bboxes using template matching per digit\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Extract original digit\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window in reconstructed image\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        # Extract aligned digit\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Compute metrics\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "# -----------------------------------\n",
    "# Compute PSNR Heatmap\n",
    "# -----------------------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "\n",
    "psnr_dict_avg = {}\n",
    "psnr_dict_worst = {}\n",
    "\n",
    "for meta_file in tqdm(metadata_files, desc=\"Processing images\", unit=\"image\"):\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    alpha, beta, noise_level = metadata['alpha'], metadata['beta'], metadata['noise_level']\n",
    "    digit_bboxes = metadata['digit_bboxes']\n",
    "\n",
    "    # Only consider moderate noise images\n",
    "    if noise_level > moderate_noise_threshold:\n",
    "        continue\n",
    "\n",
    "    idx = metadata['idx']\n",
    "    original_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    if not (os.path.exists(original_path) and os.path.exists(distorted_path)):\n",
    "        continue\n",
    "\n",
    "    # Load images as tensors\n",
    "    original_img = to_tensor(Image.open(original_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_img = model(distorted_img)\n",
    "        reconstructed_img = torch.clamp(reconstructed_img, 0, 1)\n",
    "\n",
    "    # Convert images to NumPy\n",
    "    original_np = original_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    reconstructed_np = reconstructed_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Initially compute PSNR per digit directly\n",
    "    digit_psnrs = []\n",
    "    _, _, H, W = original_img.shape\n",
    "    valid_bboxes = []\n",
    "    for (x, y, w_, h_) in digit_bboxes:\n",
    "        x, y = max(x,0), max(y,0)\n",
    "        x2, y2 = min(x+w_, W), min(y+h_, H)\n",
    "        if x2 <= x or y2 <= y:\n",
    "            continue\n",
    "        original_digit = original_img[:, :, y:y2, x:x2]\n",
    "        reconstructed_digit = reconstructed_img[:, :, y:y2, x:x2]\n",
    "        psnr_val = calculate_psnr(reconstructed_digit, original_digit)\n",
    "        digit_psnrs.append(psnr_val)\n",
    "        valid_bboxes.append((x,y,w_,h_))\n",
    "\n",
    "    # Now, always apply template matching alignment per digit\n",
    "    psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, valid_bboxes)\n",
    "    if len(psnr_per_number) == 0:\n",
    "        avg_psnr = 0\n",
    "        worst_psnr = 0\n",
    "    else:\n",
    "        avg_psnr = np.mean(psnr_per_number)\n",
    "        worst_psnr = min(psnr_per_number)\n",
    "\n",
    "    if (alpha, beta) not in psnr_dict_avg:\n",
    "        psnr_dict_avg[(alpha, beta)] = []\n",
    "        psnr_dict_worst[(alpha, beta)] = []\n",
    "    psnr_dict_avg[(alpha, beta)].append(avg_psnr)\n",
    "    psnr_dict_worst[(alpha, beta)].append(worst_psnr)\n",
    "\n",
    "# Average over multiple images if any\n",
    "for key in psnr_dict_avg:\n",
    "    psnr_dict_avg[key] = np.mean(psnr_dict_avg[key])\n",
    "    psnr_dict_worst[key] = np.mean(psnr_dict_worst[key])\n",
    "\n",
    "alpha_values = sorted(set(a for (a, b) in psnr_dict_avg.keys()))\n",
    "beta_values = sorted(set(b for (a, b) in psnr_dict_avg.keys()))\n",
    "num_alphas, num_betas = len(alpha_values), len(beta_values)\n",
    "\n",
    "psnr_matrix_avg = np.full((num_betas, num_alphas), np.nan)\n",
    "alpha_to_idx = {val: i for i, val in enumerate(alpha_values)}\n",
    "beta_to_idx = {val: i for i, val in enumerate(beta_values)}\n",
    "\n",
    "for (a, b), val in psnr_dict_avg.items():\n",
    "    psnr_matrix_avg[beta_to_idx[b], alpha_to_idx[a]] = val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image details for a given alpha,beta function\n",
    "\n",
    "# -----------------------------------\n",
    "# Function to show image details for a given alpha,beta\n",
    "# Using the same template matching approach inside show_image_details_for\n",
    "# -----------------------------------\n",
    "def show_image_details_for(alpha, beta, data_dir, model, device):\n",
    "    metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "    found_file = None\n",
    "    for meta_file in metadata_files:\n",
    "        meta_path = os.path.join(data_dir, meta_file)\n",
    "        with open(meta_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        if metadata.get('alpha') == alpha and metadata.get('beta') == beta:\n",
    "            found_file = {\n",
    "                \"metadata_file\": meta_file,\n",
    "                \"index\": metadata.get('idx'),\n",
    "                \"noise_level\": metadata.get('noise_level'),\n",
    "                \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "                \"plate_number\": metadata.get('plate_number')\n",
    "            }\n",
    "            break\n",
    "\n",
    "    if found_file is None:\n",
    "        print(f\"No images found for alpha={alpha}, beta={beta}.\")\n",
    "        return\n",
    "\n",
    "    found_file['digit_bboxes'].sort(key=lambda bbox: bbox[0])\n",
    "\n",
    "    idx = found_file['index']\n",
    "    original_image_path = os.path.join(data_dir, f\"original_{idx}.png\")\n",
    "    distorted_image_path = os.path.join(data_dir, f\"distorted_{idx}.png\")\n",
    "\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_tensor = model(distorted_img)\n",
    "        reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "    original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    # Align and update bboxes with template matching\n",
    "    def calc_psnr_ssim(original_image, reconstructed_image, digit_bboxes):\n",
    "        # We'll reuse align_and_update_bboxes here to get aligned results\n",
    "        psnr_vals, ssim_vals, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes)\n",
    "        return psnr_vals, ssim_vals, updated_bboxes\n",
    "\n",
    "    psnr_per_number, ssim_per_number, updated_bboxes = calc_psnr_ssim(original_img, reconstructed_tensor, found_file['digit_bboxes'])\n",
    "\n",
    "    reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "    reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "    original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "    for i, bbox in enumerate(found_file['digit_bboxes'], start=1):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "        cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "    for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "        cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "    original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "    reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "    for i, (p, s) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "        table_data.append([str(i), f\"{p:.2f}\", f\"{s:.3f}\"])\n",
    "    transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "    fig2 = plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(original_image_rgb)\n",
    "    plt.title(f'Original Image (Alpha={alpha}, Beta={beta})')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.imshow(reconstructed_image_rgb)\n",
    "    plt.title('Reconstructed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    table = plt.table(cellText=transposed_table_data,\n",
    "                      cellLoc='center',\n",
    "                      loc='center',\n",
    "                      bbox=[0, -0.55, 1, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "# Display Heatmap and Add Click Event\n",
    "# -----------------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(psnr_matrix_avg, origin='lower', aspect='auto', cmap=\"viridis\")\n",
    "plt.title(\"Average PSNR per Digit\")\n",
    "plt.colorbar(label='PSNR (dB)')\n",
    "plt.xticks(range(0, num_alphas, 5), alpha_values[::5])\n",
    "plt.yticks(range(0, num_betas, 5), beta_values[::5])\n",
    "plt.xlabel(\"Alpha (degrees)\")\n",
    "plt.ylabel(\"Beta (degrees)\")\n",
    "\n",
    "def format_coord(x, y):\n",
    "    col = int(round(x))\n",
    "    row = int(round(y))\n",
    "    if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "        alpha = alpha_values[col]\n",
    "        beta = beta_values[row]\n",
    "        psnr_value = psnr_matrix_avg[row, col]\n",
    "        return f\"Alpha: {alpha:.0f}, Beta: {beta:.0f}, PSNR: {psnr_value:.2f} dB\" if not np.isnan(psnr_value) else f\"Alpha: {alpha:.0f}, Beta: {beta:.0f}, PSNR: N/A\"\n",
    "    return \"Alpha: N/A, Beta: N/A\"\n",
    "\n",
    "plt.gca().format_coord = format_coord\n",
    "\n",
    "def on_click(event):\n",
    "    if event.inaxes == plt.gca():\n",
    "        x, y = event.xdata, event.ydata\n",
    "        if x is None or y is None:\n",
    "            return\n",
    "        col = int(round(x))\n",
    "        row = int(round(y))\n",
    "        if 0 <= row < num_betas and 0 <= col < num_alphas:\n",
    "            alpha = alpha_values[col]\n",
    "            beta = beta_values[row]\n",
    "            show_image_details_for(alpha, beta, data_dir, model, device)\n",
    "\n",
    "plt.gcf().canvas.mpl_connect('button_press_event', on_click)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR with Global Template Matching and Bounding Box Alignment\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------\n",
    "\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize PaddleOCR (English, digits only, low confidence allowed)\n",
    "ocr = PaddleOCR(\n",
    "    use_angle_cls=False,  \n",
    "    lang='en',\n",
    "    use_space_char=False,\n",
    "    drop_score=0.1,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Load Model via MLflow\n",
    "# -----------------------------------------------------\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Locate Metadata for Given Alpha, Beta\n",
    "# -----------------------------------------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "found_file = None\n",
    "for meta_file in metadata_files:\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "        found_file = {\n",
    "            \"metadata_file\": meta_file,\n",
    "            \"index\": metadata.get('idx'),\n",
    "            \"noise_level\": metadata.get('noise_level'),\n",
    "            \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "            \"plate_number\": metadata.get('plate_number')\n",
    "        }\n",
    "        break\n",
    "\n",
    "if not found_file:\n",
    "    print(\"No image found for given alpha, beta.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "print(f\"Noise Level: {found_file['noise_level']:.2f}\")\n",
    "print(f\"Plate Number: {found_file['plate_number']}\")\n",
    "\n",
    "# Sort original bboxes by x to ensure left-to-right order\n",
    "original_bboxes = sorted(found_file['digit_bboxes'], key=lambda bbox: bbox[0])\n",
    "\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    \"\"\"\n",
    "    Template matching to find updated bounding boxes for each digit.\n",
    "    \"\"\"\n",
    "    from pytorch_msssim import ssim\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x = max_loc[0] + search_x1\n",
    "        best_y = max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Run PaddleOCR on the reconstructed image\n",
    "# -----------------------------------------------------\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show_bgr = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "ocr_results = ocr.ocr(reconstructed_show_bgr, cls=False)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Process OCR Results:\n",
    "# Filter digits only, and align them with updated_bboxes\n",
    "# -----------------------------------------------------\n",
    "def extract_digits_from_ocr(ocr_results):\n",
    "    \"\"\"\n",
    "    Extracts per-character bounding boxes for digits from PaddleOCR line results.\n",
    "    If a line says \"3163\" with one bounding box, we split horizontally.\n",
    "    Returns a list of dicts: {\"char\": c, \"bbox\": (x1,y1,x2,y2)} for each digit found.\n",
    "    \"\"\"\n",
    "    per_char_results = []\n",
    "\n",
    "    for line in ocr_results[0]:\n",
    "        box, (text, conf) = line\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            continue\n",
    "        text = text.strip()\n",
    "\n",
    "        # Keep only digits from this text\n",
    "        filtered_text = \"\".join([ch for ch in text if ch.isdigit()])\n",
    "        if len(filtered_text) == 0:\n",
    "            continue\n",
    "\n",
    "        xs = [p[0] for p in box]\n",
    "        ys = [p[1] for p in box]\n",
    "        min_x, max_x = min(xs), max(xs)\n",
    "        min_y, max_y = min(ys), max(ys)\n",
    "\n",
    "        num_chars = len(text)\n",
    "        # We'll subdivide bounding box by number of characters to approximate char positions\n",
    "        # But we only consider digits. This means we must map digits back to their positions\n",
    "        # in the original text.\n",
    "\n",
    "        # Approach: for each character in text, assign a sub-box. Then only keep if it's a digit.\n",
    "        width = max_x - min_x\n",
    "        char_width = width / num_chars if num_chars > 0 else width\n",
    "\n",
    "        current_x = min_x\n",
    "        for i, ch in enumerate(text):\n",
    "            c_x1 = int(min_x + i * char_width)\n",
    "            c_x2 = int(min_x + (i+1)* char_width)\n",
    "            c_y1 = int(min_y)\n",
    "            c_y2 = int(max_y)\n",
    "\n",
    "            if ch.isdigit():\n",
    "                per_char_results.append({\n",
    "                    \"char\": ch,\n",
    "                    \"bbox\": (c_x1, c_y1, c_x2, c_y2)\n",
    "                })\n",
    "\n",
    "    return per_char_results\n",
    "\n",
    "char_results = []\n",
    "if ocr_results and len(ocr_results) > 0 and ocr_results[0] is not None:\n",
    "    char_results = extract_digits_from_ocr(ocr_results)\n",
    "\n",
    "# Sort recognized digits by x-coordinate\n",
    "char_results.sort(key=lambda r: r[\"bbox\"][0])  # sort by left x\n",
    "\n",
    "recognized_chars = [r[\"char\"] for r in char_results]\n",
    "recognized_text = \"\".join(recognized_chars)\n",
    "\n",
    "# We know we need exactly 6 digits:\n",
    "plate_number = found_file['plate_number']\n",
    "assert len(plate_number) == 6, \"Plate number must have exactly 6 digits\"\n",
    "\n",
    "# If we have more than 6 recognized digits, take only first 6 (leftmost)\n",
    "if len(recognized_chars) > 6:\n",
    "    recognized_chars = recognized_chars[:6]\n",
    "    char_results = char_results[:6]\n",
    "\n",
    "# If fewer than 6 digits recognized, fill missing with '?'\n",
    "if len(recognized_chars) < 6:\n",
    "    missing = 6 - len(recognized_chars)\n",
    "    recognized_chars.extend(['?'] * missing)\n",
    "\n",
    "final_recognized_text = \"\".join(recognized_chars[:6])\n",
    "\n",
    "# Compute accuracy:\n",
    "correct_digits = sum(1 for a, b in zip(plate_number, final_recognized_text) if a == b)\n",
    "accuracy = correct_digits / 6.0\n",
    "\n",
    "print(f\"\\nPlate Number (GT): {plate_number}\")\n",
    "print(f\"Recognized (final): {final_recognized_text}\")\n",
    "print(f\"OCR Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Visualization\n",
    "# -----------------------------------------------------\n",
    "# Draw original bounding boxes on original image\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "reconstructed_show_bgr = reconstructed_show_bgr.copy()\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show_bgr, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show_bgr, str(i), (x, y-5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw recognized characters on reconstructed image\n",
    "for r in char_results:\n",
    "    c = r[\"char\"]\n",
    "    x1, y1, x2, y2 = r[\"bbox\"]\n",
    "    cv2.rectangle(reconstructed_show_bgr, (x1,y1), (x2,y2), (0,255,0), 1)\n",
    "    cv2.putText(reconstructed_show_bgr, c, (x1,y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0),1)\n",
    "\n",
    "# Prepare table for PSNR, SSIM\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image with Original BBoxes')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title(f\"Reconstructed Image with Updated BBoxes and OCR Results\\nRecognized: {final_recognized_text}, Accuracy: {accuracy*100:.2f}%\")\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "\n",
    "plt.subplots_adjust(left=0.2, bottom=0.3, top=0.9)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Digit OCR with Template Matching and Margin Adjustment\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from paddleocr import PaddleOCR\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# --------------------\n",
    "# Configuration\n",
    "# --------------------\n",
    "\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "search_margin = 10  # Margin for template matching search\n",
    "ocr_margin = 17      # Margin around TMBB for OCR\n",
    "\n",
    "digit_dict = \"digit_dict.txt\"\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(\n",
    "    use_angle_cls=True,\n",
    "    lang='en',\n",
    "    rec_char_dict_path=digit_dict,\n",
    "    use_space_char=False,\n",
    "    drop_score=0\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Load the Model\n",
    "# --------------------\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "runs = client.search_runs(experiment_ids=experiment.experiment_id, order_by=[\"attributes.start_time DESC\"])\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval().to(device)\n",
    "\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n",
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Find Matching alpha, beta\n",
    "# --------------------\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "found_file = None\n",
    "for meta_file in metadata_files:\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "        found_file = {\n",
    "            \"metadata_file\": meta_file,\n",
    "            \"index\": metadata.get('idx'),\n",
    "            \"noise_level\": metadata.get('noise_level'),\n",
    "            \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "            \"plate_number\": metadata.get('plate_number')\n",
    "        }\n",
    "        break\n",
    "\n",
    "if not found_file:\n",
    "    print(\"Image with the specified alpha and beta not found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "print(f\"Noise Level: {found_file['noise_level']:.2f}\")\n",
    "print(f\"Plate Number: {found_file['plate_number']}\")\n",
    "\n",
    "original_bboxes = sorted(found_file['digit_bboxes'], key=lambda bbox: bbox[0])\n",
    "\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    from pytorch_msssim import ssim\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Search region\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "# Draw original bounding boxes\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# --------------------\n",
    "# Per-Digit OCR using TMBB and PaddleOCR\n",
    "# --------------------\n",
    "def run_ocr_on_digit(reconstructed_bgr, bbox, margin=5):\n",
    "    x, y, w, h = bbox\n",
    "    H, W, _ = reconstructed_bgr.shape\n",
    "    x1 = max(0, x - margin)\n",
    "    y1 = max(0, y - margin)\n",
    "    x2 = min(W, x + w + margin)\n",
    "    y2 = min(H, y + h + margin)\n",
    "\n",
    "    digit_roi = reconstructed_bgr[y1:y2, x1:x2]\n",
    "    if digit_roi.size == 0:\n",
    "        print(f\"Warning: Empty ROI for bbox {bbox}\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"\\nRunning OCR on region: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "\n",
    "    try:\n",
    "        ocr_results = ocr.ocr(digit_roi, cls=False)\n",
    "        print(f\"OCR Raw Results: {ocr_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OCR: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    recognized_digit = \"\"\n",
    "    if ocr_results and ocr_results[0]:\n",
    "        for line in ocr_results[0]:\n",
    "            box, text_data = line[0], line[1]\n",
    "            recognized_text, confidence = text_data\n",
    "            filtered = \"\".join(ch for ch in recognized_text if ch.isdigit())\n",
    "            if filtered:\n",
    "                recognized_digit = filtered[0]\n",
    "                print(f\"  Detected: '{recognized_digit}' (Confidence: {confidence:.2f})\")\n",
    "                break\n",
    "\n",
    "    if recognized_digit:\n",
    "        print(f\"Final Recognized Digit: {recognized_digit}\")\n",
    "    else:\n",
    "        print(\"No valid digits recognized.\")\n",
    "    return recognized_digit\n",
    "\n",
    "\n",
    "\n",
    "recognized_digits = []\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    digit = run_ocr_on_digit(reconstructed_show, bbox, margin=ocr_margin)\n",
    "    if not digit:\n",
    "        digit = \"?\"  # If no digit recognized, use placeholder\n",
    "    recognized_digits.append(digit)\n",
    "\n",
    "recognized_text = \"\".join(recognized_digits)\n",
    "ground_truth = found_file['plate_number']\n",
    "correct_digits = sum(1 for a, b in zip(ground_truth, recognized_text) if a == b)\n",
    "ocr_accuracy = correct_digits / len(ground_truth) if ground_truth else 0.0\n",
    "\n",
    "print(f\"Recognized Digits (Left to Right): {recognized_text}\")\n",
    "print(f\"OCR Accuracy: {ocr_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Draw per-digit OCR results\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    digit = recognized_digits[i-1]\n",
    "    cv2.putText(reconstructed_image_rgb, digit, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "# --------------------\n",
    "# Visualization\n",
    "# --------------------\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title(f\"Reconstructed Image - OCR: {recognized_text}, Accuracy: {ocr_accuracy*100:.2f}%\")\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_alpha = 88  # Target alpha\n",
    "target_beta = 12  # Target beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit-Level OCR with Template Matching and Tesseract Integration\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract\n",
    "from pytorch_msssim import ssim\n",
    "import torch.nn.functional as F\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# --------------------\n",
    "# Configuration\n",
    "# --------------------\n",
    "data_dir = \"data_test\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set the MLflow experiment and load the model\n",
    "mlflow.set_experiment('Unet_Final')\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Unet_Final')\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=experiment.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs[0].info.run_id\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded from run {run_id} in experiment '{experiment.name}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_psnr(outputs, targets):\n",
    "    mse = F.mse_loss(outputs, targets)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "# Find metadata for the target alpha, beta\n",
    "metadata_files = [f for f in os.listdir(data_dir) if f.startswith('metadata_') and f.endswith('.json')]\n",
    "found_file = None\n",
    "for meta_file in metadata_files:\n",
    "    meta_path = os.path.join(data_dir, meta_file)\n",
    "    with open(meta_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    if metadata.get('alpha') == target_alpha and metadata.get('beta') == target_beta:\n",
    "        found_file = {\n",
    "            \"metadata_file\": meta_file,\n",
    "            \"index\": metadata.get('idx'),\n",
    "            \"noise_level\": metadata.get('noise_level'),\n",
    "            \"digit_bboxes\": metadata.get('digit_bboxes'),\n",
    "            \"plate_number\": metadata.get('plate_number')\n",
    "        }\n",
    "        break\n",
    "\n",
    "if not found_file:\n",
    "    print(\"Image with the specified alpha and beta not found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found metadata file: {found_file['metadata_file']}\")\n",
    "print(f\"Alpha: {target_alpha}, Beta: {target_beta}\")\n",
    "print(f\"Noise Level: {found_file['noise_level']:.2f}\")\n",
    "print(f\"Plate Number: {found_file['plate_number']}\")\n",
    "\n",
    "original_bboxes = sorted(found_file['digit_bboxes'], key=lambda bbox: bbox[0])\n",
    "\n",
    "original_image_path = os.path.join(data_dir, f\"original_{found_file['index']}.png\")\n",
    "distorted_image_path = os.path.join(data_dir, f\"distorted_{found_file['index']}.png\")\n",
    "\n",
    "original_img = to_tensor(Image.open(original_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "distorted_img = to_tensor(Image.open(distorted_image_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_tensor = model(distorted_img)\n",
    "    reconstructed_tensor = torch.clamp(reconstructed_tensor, 0.0, 1.0)\n",
    "\n",
    "original_np = original_img.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "# Align and update bboxes using template matching\n",
    "def align_and_update_bboxes(original_np, reconstructed_np, digit_bboxes):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    updated_bboxes = []\n",
    "    search_margin = 10\n",
    "\n",
    "    for bbox in digit_bboxes:\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Extract original digit\n",
    "        original_digit = original_np[y:y+h, x:x+w, :]\n",
    "        original_digit_gray = cv2.cvtColor((original_digit * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Define search window in reconstructed\n",
    "        search_x1 = max(0, x - search_margin)\n",
    "        search_y1 = max(0, y - search_margin)\n",
    "        search_x2 = min(reconstructed_np.shape[1], x + w + search_margin)\n",
    "        search_y2 = min(reconstructed_np.shape[0], y + h + search_margin)\n",
    "        search_region = reconstructed_np[search_y1:search_y2, search_x1:search_x2, :]\n",
    "        search_region_gray = cv2.cvtColor((search_region * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region_gray, original_digit_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        best_x, best_y = max_loc[0] + search_x1, max_loc[1] + search_y1\n",
    "\n",
    "        updated_bboxes.append((best_x, best_y, w, h))\n",
    "\n",
    "        # Compute PSNR and SSIM\n",
    "        aligned_digit = reconstructed_np[best_y:best_y+h, best_x:best_x+w, :]\n",
    "        original_digit_tensor = torch.from_numpy(original_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "        aligned_digit_tensor = torch.from_numpy(aligned_digit.transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "\n",
    "        psnr_val = calculate_psnr(aligned_digit_tensor, original_digit_tensor)\n",
    "        ssim_val = ssim(aligned_digit_tensor, original_digit_tensor, data_range=1.0, size_average=True).item()\n",
    "        psnr_values.append(psnr_val)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "    return psnr_values, ssim_values, updated_bboxes\n",
    "\n",
    "psnr_per_number, ssim_per_number, updated_bboxes = align_and_update_bboxes(original_np, reconstructed_np, original_bboxes)\n",
    "\n",
    "# Convert reconstructed to BGR for visualization\n",
    "reconstructed_show = (reconstructed_np * 255).astype(np.uint8)\n",
    "reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "original_image_cv = cv2.imread(original_image_path)\n",
    "\n",
    "# Draw original bounding boxes\n",
    "for i, bbox in enumerate(original_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(original_image_cv, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(original_image_cv, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "# Draw updated bounding boxes on reconstructed image\n",
    "for i, bbox in enumerate(updated_bboxes, start=1):\n",
    "    x, y, w, h = bbox\n",
    "    cv2.rectangle(reconstructed_show, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "    cv2.putText(reconstructed_show, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 150, 0), 1)\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image_cv, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Tesseract single-digit OCR function\n",
    "def ocr_single_digit(image_bgr, margin=5):\n",
    "    \"\"\"\n",
    "    Recognize a single digit from a small cropped patch using Tesseract.\n",
    "    Uses --psm 10 for single char and a whitelist of digits.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    # Binary threshold might help\n",
    "    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Tesseract config for single character digit recognition\n",
    "    config = r'--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789'\n",
    "\n",
    "    text = pytesseract.image_to_string(thresh, config=config).strip()\n",
    "    if len(text) == 1 and text.isdigit():\n",
    "        return text\n",
    "    # If Tesseract fails or returns something unexpected, return '?'\n",
    "    return '?'\n",
    "\n",
    "# Perform per-digit OCR using TMBB\n",
    "plate_number_gt = found_file['plate_number']\n",
    "recognized_digits = []\n",
    "\n",
    "for i, bbox in enumerate(updated_bboxes):\n",
    "    x, y, w, h = bbox\n",
    "    # Add margin\n",
    "    M = 16\n",
    "    x1 = max(0, x - M)\n",
    "    y1 = max(0, y - M)\n",
    "    x2 = min(reconstructed_show.shape[1], x + w + M)\n",
    "    y2 = min(reconstructed_show.shape[0], y + h + M)\n",
    "    digit_patch = reconstructed_show[y1:y2, x1:x2]\n",
    "    \n",
    "    reconstructed_show = cv2.cvtColor(reconstructed_show, cv2.COLOR_RGB2BGR)\n",
    "    cv2.rectangle(reconstructed_show, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "    reconstructed_image_rgb = cv2.cvtColor(reconstructed_show, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    recognized_digit = ocr_single_digit(digit_patch)\n",
    "    recognized_digits.append(recognized_digit)\n",
    "\n",
    "recognized_text = \"\".join(recognized_digits)\n",
    "\n",
    "# Compute accuracy\n",
    "gt = plate_number_gt\n",
    "correct_digits = sum(1 for a, b in zip(gt, recognized_text) if a == b)\n",
    "accuracy = correct_digits / len(gt) if gt else 0.0\n",
    "\n",
    "# Prepare table\n",
    "table_data = [[\"Digit\", \"PSNR(dB)\", \"SSIM\"]]\n",
    "for i, (psnr_val, ssim_val) in enumerate(zip(psnr_per_number, ssim_per_number), start=1):\n",
    "    table_data.append([str(i), f\"{psnr_val:.2f}\", f\"{ssim_val:.3f}\"])\n",
    "transposed_table_data = list(zip(*table_data))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(reconstructed_image_rgb)\n",
    "plt.title(f'Reconstructed Image\\nGT: {gt}, Recognized: {recognized_text}, Acc: {accuracy*100:.2f}%')\n",
    "plt.axis('off')\n",
    "\n",
    "table = plt.table(cellText=transposed_table_data,\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, -0.55, 1, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
